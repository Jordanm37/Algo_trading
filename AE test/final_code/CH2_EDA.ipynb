{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import warnings\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from astropy.timeseries import LombScargle\n",
    "\n",
    "from scipy.signal import welch\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf, kpss,q_stat\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy.stats import probplot, moment\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "from scipy.stats import genpareto\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(title, plot_name, fig=None):\n",
    "    if not fig:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    # Create the EDA directory if it doesn't exist\n",
    "    if not os.path.exists('EDA'):\n",
    "        os.makedirs('EDA')\n",
    "\n",
    "    # Create the subdirectory for the series if it doesn't exist\n",
    "    series_path = f'EDA/{title}'\n",
    "    if not os.path.exists(series_path):\n",
    "        os.makedirs(series_path)\n",
    "\n",
    "    # Save the plot in the corresponding subdirectory with the plot_name in the file name\n",
    "    fig.savefig(f'{series_path}/{plot_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Decompose the given time series into trend, seasonal, and residual components\n",
    "\n",
    "\n",
    "def decompose(series, frequency, title, figsize=(20, 12)):\n",
    "    frequency_int = {\n",
    "        'H': 24,  # hours in a day\n",
    "        'D': 7,   # days in a week\n",
    "        'W': 52,  # weeks in a year\n",
    "        'M': 12,  # months in a year\n",
    "    }\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=1, figsize=figsize)\n",
    "\n",
    "    decomposition = STL(series, period=frequency_int[frequency], robust=True)\n",
    "    result = decomposition.fit()\n",
    "    trend = result.trend\n",
    "    seasonal = result.seasonal\n",
    "    residual = result.resid\n",
    "\n",
    "    axs[0].plot(series)\n",
    "    axs[0].set_ylabel('Observed')\n",
    "\n",
    "    axs[1].plot(trend)\n",
    "    axs[1].set_ylabel('Trend')\n",
    "\n",
    "    axs[2].plot(seasonal)\n",
    "    axs[2].set_ylabel('Seasonal')\n",
    "\n",
    "    axs[3].scatter(series.index, residual)\n",
    "    axs[3].set_ylabel('Residual')\n",
    "\n",
    "    fig.suptitle(title+' '+frequency, fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    save_plot(series.name, f'seasonal_decomposition_{frequency}')\n",
    "\n",
    "\n",
    "def plot_power_spectral_density(ts, frequency, title):\n",
    "    freqs, psd = welch(ts)\n",
    "    plt.plot(freqs, psd)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Power Spectral Density')\n",
    "    plt.title(title)\n",
    "\n",
    "    save_plot(ts.name, f'power_spectral_density_{frequency}')\n",
    "\n",
    "\n",
    "def plot_lomb_scargle_periodogram(ts, frequency, title):\n",
    "    time = np.arange(len(ts))\n",
    "    ls = LombScargle(time, ts)\n",
    "    freq, power = ls.autopower()\n",
    "    plt.plot(freq, power)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(title)\n",
    "\n",
    "    save_plot(ts.name, f'lomb_scargle_periodogram_{frequency}')\n",
    "\n",
    "\n",
    "def plot_acf_pacf(data, frequency, title, max_lags=None):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    # Plot ACF\n",
    "    plot_acf(data, lags=max_lags, ax=axes[0], alpha=0.05)\n",
    "    axes[0].set_ylabel('ACF')\n",
    "\n",
    "    # Plot PACF\n",
    "    plot_pacf(data, lags=max_lags, ax=axes[1], alpha=0.05)\n",
    "    axes[1].set_xlabel('Lag')\n",
    "    axes[1].set_ylabel('PACF')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    save_plot(data.name, f'acf_pacf_{frequency}', fig)\n",
    "\n",
    "\n",
    "def autocorrelation_test(series, lags=10):\n",
    "    lags = min(10, int(series.size)/5) if lags is None else lags\n",
    "    if series.empty or series.size < lags:\n",
    "        print(\n",
    "            f\"Cannot perform Ljung-Box test: series is empty or has less than {lags} observations.\")\n",
    "        return None\n",
    "\n",
    "    stats = sm.stats.acorr_ljungbox(series, lags=lags, auto_lag=True)\n",
    "    return stats['lb_pvalue']\n",
    "\n",
    "\n",
    "def spectral_analysis(series, title, freq_list=['H', 'D', 'W', 'M']):\n",
    "    # Loop over each frequency\n",
    "    for freq in freq_list:\n",
    "        # Resample original data\n",
    "        resampled_data = series.resample(freq).mean()\n",
    "        print(len(resampled_data))\n",
    "        print('Resampled data for frequency:', freq)\n",
    "        # Analyse power spectral density and Lomb-Scargle periodogram\n",
    "        # plot_power_spectral_density(resampled_data,freq, title)\n",
    "        plot_lomb_scargle_periodogram(resampled_data, freq, title)\n",
    "        # Analyse ACF and PACF plots\n",
    "        plot_acf_pacf(resampled_data, freq, title)\n",
    "        try:\n",
    "            autocorr_test_result = autocorrelation_test(\n",
    "                resampled_data, lags=10)\n",
    "            if autocorr_test_result is not None:\n",
    "                print(autocorr_test_result)\n",
    "            else:\n",
    "                print(\"The input series is empty.\")\n",
    "        except:\n",
    "            print(\"The input series is empty.\")\n",
    "\n",
    "\n",
    "def display_data_statistics(df):\n",
    "    print(\"Mean:\\n\", df.mean())\n",
    "    print(\"Variance:\\n\", df.var())\n",
    "    print(\"Covariance:\\n\", df.cov())\n",
    "\n",
    "\n",
    "def analyse_kolmogorov_smirnov(df):\n",
    "    for col in df.columns:\n",
    "        h, p = stats.kstest(df[col], 'norm')\n",
    "        if p < 0.05:\n",
    "            print(\n",
    "                f\"Kolmogorov-Smirnov test for '{col}': The distribution is not normal (p-value = {p})\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Kolmogorov-Smirnov test for '{col}': The distribution is normal (p-value = {p})\")\n",
    "\n",
    "\n",
    "def analyse_skewness_kurtosis(df):\n",
    "    for col in df.columns:\n",
    "        skew = stats.skew(df[col])\n",
    "        kurt = stats.kurtosis(df[col])\n",
    "        print(f'Skewness for {col}: {skew}')\n",
    "        print(f'Kurtosis for {col}: {kurt}')\n",
    "\n",
    "\n",
    "def analyse_normality(df):\n",
    "    analyse_kolmogorov_smirnov(df)\n",
    "    analyse_skewness_kurtosis(df)\n",
    "\n",
    "\n",
    "def plot_rolling_statistics(ts, title, window=120, alpha=0.6):\n",
    "    rolmean = ts.rolling(window=window).mean()\n",
    "    rolstd = ts.rolling(window=window).std()\n",
    "\n",
    "    plt.plot(ts, color='blue', label='Original', alpha=0.5)\n",
    "    plt.plot(rolmean, color='orange', label='Rolling Mean', alpha=0.8)\n",
    "    plt.plot(rolstd, color='green', label='Rolling Std', alpha=0.6)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f'Rolling Mean and Standard Deviation of {title}')\n",
    "    save_plot(ts.name, f'Rolling Mean and Standard Deviation_{title}')\n",
    "\n",
    "\n",
    "def adf_test_fast(ts):\n",
    "    return adfuller(ts)\n",
    "\n",
    "\n",
    "def kpss_test_fast(ts):\n",
    "    return kpss(ts)\n",
    "\n",
    "\n",
    "def test_stationarity(ts):\n",
    "    print('Results of Augmented Dickey-Fuller Test:')\n",
    "    adf_test_results = adf_test_fast(ts.values)\n",
    "    adf_output = pd.Series(adf_test_results[0:4], index=[\n",
    "                           'Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for key, value in adf_test_results[4].items():\n",
    "        adf_output[f'Critical Value ({key})'] = value\n",
    "    print(adf_output)\n",
    "\n",
    "    print('\\nResults of Kwiatkowski-Phillips-Schmidt-Shin Test:')\n",
    "    kpss_test_results = kpss_test_fast(ts.values)\n",
    "    kpss_output = pd.Series(kpss_test_results[0:3], index=[\n",
    "                            'Test Statistic', 'p-value', 'Lags Used'])\n",
    "    for key, value in kpss_test_results[3].items():\n",
    "        kpss_output[f'Critical Value ({key})'] = value\n",
    "    print(kpss_output)\n",
    "\n",
    "    if adf_test_results[1] < 0.05:\n",
    "        print(\n",
    "            f\"The time series '{ts.name}' is stationary based on the Augmented Dickey-Fuller test.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"The time series '{ts.name}' is not stationary based on the Augmented Dickey-Fuller test.\")\n",
    "\n",
    "    if kpss_test_results[1] < 0.05:\n",
    "        print(\n",
    "            f\"The time series '{ts.name}' is not stationary based on the Kwiatkowski-Phillips-Schmidt-Shin test.\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"The time series '{ts.name}' is stationary based on the Kwiatkowski-Phillips-Schmidt-Shin test.\")\n",
    "\n",
    "\n",
    "def plot_hist_kde(data, cutoff_percentile=99.99):\n",
    "    cutoff_value = np.percentile(data, cutoff_percentile)\n",
    "    filtered_series = data[data <= cutoff_value]\n",
    "\n",
    "    # Histogram with smaller bins\n",
    "    fig_hist, ax_hist = plt.subplots(figsize=(10, 7))\n",
    "    sns.histplot(data=filtered_series, kde=True, bins=100, ax=ax_hist)\n",
    "    ax_hist.set_xlabel('')\n",
    "    ax_hist.set_title(f'Histogram of {data.name}')\n",
    "    ax_hist.grid()\n",
    "    ax_hist.set_ylim(0, ax_hist.get_ylim()[1] * 0.17)  # Rescale the y-axis\n",
    "    save_plot(data.name, f'Histogram_{data.name}')\n",
    "\n",
    "\n",
    "def plot_box_daily(df):\n",
    "    df_ = df.copy()\n",
    "    df_ = df_.resample('D').mean()\n",
    "    df_.boxplot(grid=False)\n",
    "    plt.title(\"Daily log-returns\")\n",
    "    save_plot(\"\", f'Daily log-returns Box plot')\n",
    "\n",
    "\n",
    "def autocorrelation_plots(data, lags):\n",
    "    columns = [data]\n",
    "    for i in range(1, lags + 1):\n",
    "        columns.append(data.shift(i))\n",
    "\n",
    "    dataframe = pd.concat(columns, axis=1)\n",
    "    column_names = ['t']\n",
    "    for i in range(1, lags + 1):\n",
    "        column_names.append('t-' + str(i))\n",
    "    dataframe.columns = column_names\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(1, lags + 1):\n",
    "        ax = plt.subplot(2, 4, i)\n",
    "        ax.set_title('t vs t-' + str(i))\n",
    "        plt.scatter(x=dataframe['t'].values, y=dataframe['t-' + str(i)].values)\n",
    "\n",
    "    save_plot(data.name, f'Autocorrelation plots_{data.name}')\n",
    "\n",
    "\n",
    "def assess_volatility_clustering(ts):\n",
    "    squared_returns = ts.pow(2)\n",
    "    absolute_returns = np.abs(ts)\n",
    "\n",
    "    perform_arch_lm_test(squared_returns, \"Squared Returns\")\n",
    "    perform_ljung_box_test(squared_returns, \"Squared Returns\")\n",
    "\n",
    "    perform_arch_lm_test(absolute_returns, \"Absolute Returns\")\n",
    "    perform_ljung_box_test(absolute_returns, \"Absolute Returns\")\n",
    "\n",
    "\n",
    "def perform_arch_lm_test(returns, return_type):\n",
    "    test_stat, p_value, f_stat, f_p_value = het_arch(returns)\n",
    "    if p_value < 0.05:\n",
    "        print(\n",
    "            f\"ARCH-LM test ({return_type}): p-value = {p_value:.4f} < 0.05, reject null hypothesis of homoskedasticity.\\n\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"ARCH-LM test ({return_type}): p-value = {p_value:.4f} >= 0.05, fail to reject null hypothesis of homoskedasticity.\\n\")\n",
    "\n",
    "\n",
    "def perform_ljung_box_test(returns, return_type):\n",
    "    try:\n",
    "        ljung_resid = autocorrelation_test(returns)\n",
    "        print(f'Ljung-Box test for {return_type} Residuals:', ljung_resid)\n",
    "        print('\\n')\n",
    "    except:\n",
    "        print(\n",
    "            f'Ljung-Box test for {return_type} Residuals: Cannot be computed.')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "def plot_volatility_clustering(ts):\n",
    "    squared_returns = ts**2\n",
    "    absolute_returns = np.abs(ts)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    axs[0].plot(squared_returns)\n",
    "    axs[0].set_xlabel('Time')\n",
    "    axs[0].set_title(\"Squared Returns\")\n",
    "    axs[1].plot(absolute_returns)\n",
    "    axs[1].set_xlabel('Time')\n",
    "    axs[1].set_title(\"Absolute Returns\")\n",
    "    save_plot(ts.name, f'Volatility Clustering_{ts.name}')\n",
    "\n",
    "\n",
    "def print_series_comparison_info(s1, s2, s1_outliers=None, s2_outliers=None):\n",
    "    # Calculate the number of non-NaN values in each series\n",
    "    s1_count = s1.value_counts().sum()\n",
    "    s2_count = s2.value_counts().sum()\n",
    "\n",
    "    # Calculate the number of non-NaN values in each outlier series\n",
    "    if s1_outliers is not None:\n",
    "        s1_outliers_count = s1_outliers.value_counts().sum()\n",
    "    else:\n",
    "        s1_outliers_count = 0\n",
    "\n",
    "    if s2_outliers is not None:\n",
    "        s2_outliers_count = s2_outliers.value_counts().sum()\n",
    "    else:\n",
    "        s2_outliers_count = 0\n",
    "\n",
    "    # Calculate the number of non-NaN values in the intersection of the outlier series\n",
    "    if s1_outliers is not None and s2_outliers is not None:\n",
    "        intersection = intersect_non_nan(s1_outliers, s2_outliers)\n",
    "        intersection_count = intersection.value_counts().sum()\n",
    "        plot_series_with_outliers(s1, s1, intersection, 'Series 1')\n",
    "        plot_series_with_outliers(s2, s2, intersection, 'Series 2')\n",
    "\n",
    "        # Print the intersection times\n",
    "        print(\"Intersection times:\")\n",
    "        print(intersection.dropna())\n",
    "    else:\n",
    "        intersection_count = 0\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Series 1 non-NaN count: {s1_count}\")\n",
    "    print(f\"Series 2 non-NaN count: {s2_count}\")\n",
    "    print(f\"Series 1 outliers count: {s1_outliers_count}\")\n",
    "    print(f\"Series 2 outliers count: {s2_outliers_count}\")\n",
    "    print(f\"Intersection of outlier series count: {intersection_count}\")\n",
    "\n",
    "\n",
    "def intersect_non_nan(s1, s2):\n",
    "    \"\"\"Return the intersection of two series, ignoring NaN values.\"\"\"\n",
    "    return s1.loc[s1.notna()].index.intersection(s2.loc[s2.notna()].index)\n",
    "\n",
    "\n",
    "def standardize_series(series):\n",
    "    return StandardScaler().fit_transform(series.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "def apply_dbscan_clustering(series_standardized, eps=0.5, min_samples=2):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    return dbscan.fit_predict(series_standardized)\n",
    "\n",
    "\n",
    "def extract_outliers_from_clusters(series, clusters):\n",
    "    \"\"\"Identify the outliers (cluster label -1 indicates an outlier)\"\"\"\n",
    "    return series[clusters == -1]\n",
    "\n",
    "\n",
    "def cluster_based_outlier_detection(series):\n",
    "    series_standardized = standardize_series(series)\n",
    "    clusters = apply_dbscan_clustering(series_standardized, eps=0.15)\n",
    "    outliers = extract_outliers_from_clusters(series, clusters)\n",
    "    return outliers\n",
    "\n",
    "\n",
    "def plot_series_with_outliers(series, detection_series, outliers, title):\n",
    "    plt.plot(series)\n",
    "    outlier_indices = detection_series[detection_series.isin(outliers)].index\n",
    "    plt.scatter(outlier_indices,\n",
    "                series.loc[outlier_indices], color='red', marker='o', label='Outliers')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title(f'{title} with Outliers Highlighted')\n",
    "    save_plot(series.name, f'Outliers_{title}')\n",
    "\n",
    "\n",
    "def peaks_over_threshold(series, threshold_quantile=0.95):\n",
    "    # Choose a high threshold using the quantile\n",
    "    threshold = np.quantile(series, threshold_quantile)\n",
    "    # Identify the exceedances and their corresponding times\n",
    "    exceedances = series[np.abs(series) >= threshold]\n",
    "    exceedance_times = series.index[series >= threshold]\n",
    "    # Fit a Generalized Pareto Distribution (GPD) to the exceedances above the threshold\n",
    "    shape, loc, scale = genpareto.fit(exceedances - threshold)\n",
    "    # Return the threshold, exceedances, times, and GPD parameters\n",
    "    return threshold, exceedances, exceedance_times, (shape, loc, scale)\n",
    "\n",
    "\n",
    "def peaks_over_threshold_outlier_detection(series):\n",
    "    thresholds = peaks_over_threshold(series, threshold_quantile=0.999)[1]\n",
    "    plot_series_with_outliers(series, thresholds, series.name)\n",
    "\n",
    "\n",
    "def analyse_seasonality(series, title, freq_list=['H', 'D', 'W', 'M']):\n",
    "    # Loop over each frequency\n",
    "    for freq in freq_list:\n",
    "        # Resample original data\n",
    "        resampled_data = series.resample(freq).mean()\n",
    "\n",
    "        # Perform seasonal decomposition\n",
    "        decompose(resampled_data, freq, title)\n",
    "\n",
    "\n",
    "def analyse_distribution(df):\n",
    "    # Display data statistics\n",
    "    print(\"Data statistics:\")\n",
    "    display_data_statistics(df)\n",
    "\n",
    "    ONE_HOUR = 120\n",
    "    # Plot rolling statistics\n",
    "    for col in df.columns:\n",
    "        print(f\"Rolling statistics of {col}:\")\n",
    "        plot_rolling_statistics(df[col], col, window=ONE_HOUR)\n",
    "\n",
    "    # Plot box, histogram, and KDE\n",
    "    for col in df.columns:\n",
    "        print(f\"Distribution analysis of {col}:\")\n",
    "        plot_hist_kde(df[col])\n",
    "\n",
    "    # plot boxplot\n",
    "    plot_box_daily(df)\n",
    "\n",
    "    # analyse normality\n",
    "    print(\"Normality analysis:\")\n",
    "    analyse_normality(df)\n",
    "\n",
    "    # Test stationarity\n",
    "    for col in df.columns:\n",
    "        print(f\"Stationarity of {col}:\")\n",
    "        test_stationarity(df[col])\n",
    "\n",
    "\n",
    "def assess_and_plot_volatility_clustering(rt):\n",
    "    plot_volatility_clustering(rt)\n",
    "    assess_volatility_clustering(rt)\n",
    "\n",
    "\n",
    "def plot_series_and_returns(ts, title):\n",
    "    # Calculate log returns and squared log returns\n",
    "    returns = calculate_log_returns(ts)\n",
    "    squared_log_returns = returns**2\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15))\n",
    "\n",
    "    # Plot time series\n",
    "    axes[0].plot(ts)\n",
    "    axes[0].set_title(f'{title} - Time Series')\n",
    "    axes[0].set_xlabel('Time')\n",
    "    axes[0].set_ylabel('Value')\n",
    "\n",
    "    # Plot log returns\n",
    "    axes[1].plot(returns)\n",
    "    axes[1].set_title(f'{title} - Log Returns')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].set_ylabel('Log Returns')\n",
    "\n",
    "    # Plot squared log returns\n",
    "    axes[2].plot(squared_log_returns)\n",
    "    axes[2].set_title(f'{title} - Squared Log Returns')\n",
    "    axes[2].set_xlabel('Time')\n",
    "    axes[2].set_ylabel('Squared Log Returns')\n",
    "\n",
    "    # Adjust layout and display plot\n",
    "    plt.tight_layout()\n",
    "    save_plot(title, f'Series Plots_{title}')\n",
    "\n",
    "\n",
    "def plot_normalised_price(ts_1, ts_2):\n",
    "    df = pd.concat([ts_1, ts_2], axis=1)\n",
    "    scaled_df = scale(df)\n",
    "    scaled_df.columns = ['ts1', 'ts2']\n",
    "    scaled_df.plot()\n",
    "    plt.title('Prices scaled')\n",
    "    save_plot(\"\", f'Normliased Prices')\n",
    "\n",
    "\n",
    "def analyse_outliers(rt1, rt2, frequency):\n",
    "    # Calculate squared returns for both series\n",
    "    squared_rt1 = rt1.pow(2)\n",
    "    squared_rt2 = rt2.pow(2)\n",
    "\n",
    "    # Perform cluster-based outlier detection for both series\n",
    "    outliers_rt1 = cluster_based_outlier_detection(squared_rt1)\n",
    "    outliers_rt2 = cluster_based_outlier_detection(squared_rt2)\n",
    "\n",
    "    # Print the number of outliers detected in the volatility of both series\n",
    "    print(f\"Number of outliers in volatility of ts1: {len(outliers_rt1)}\")\n",
    "    print(f\"Number of outliers in volatility of ts2: {len(outliers_rt2)}\")\n",
    "\n",
    "    # Plot returns with outliers calculated from the squared returns for both series\n",
    "    plot_series_with_outliers(\n",
    "        rt1, squared_rt1, outliers_rt1, f'{rt1.name} {frequency} Returns')\n",
    "    plot_series_with_outliers(\n",
    "        rt2, squared_rt2, outliers_rt2, f'{rt2.name} {frequency} Returns')\n",
    "\n",
    "    # Plot squared returns with outliers for both series\n",
    "    plot_series_with_outliers(squared_rt1, squared_rt1,\n",
    "                              outliers_rt1, f'{rt1.name} {frequency} Volatility')\n",
    "    plot_series_with_outliers(squared_rt2, squared_rt2,\n",
    "                              outliers_rt2, f'{rt2.name} {frequency} Volatility')\n",
    "\n",
    "    # check for common outliers\n",
    "    print_series_comparison_info(rt1, rt2, outliers_rt1, outliers_rt2)\n",
    "\n",
    "\n",
    "def analyse_time_series(ts1, ts2, rt1, rt2):\n",
    "    print(\"Seasonality analysis for ts1:\")\n",
    "    analyse_seasonality(ts1, 'ts1', freq_list=['H', 'D', 'W'])\n",
    "\n",
    "    print(\"Seasonality analysis for ts2:\")\n",
    "    analyse_seasonality(ts2, 'ts2', freq_list=['H', 'D', 'W'])\n",
    "\n",
    "    print(\"Spectral analysis for rt1:\")\n",
    "    spectral_analysis(rt1, 'ts1', freq_list=['H', 'D', 'W'])\n",
    "\n",
    "    print(\"Spectral analysis for rt2:\")\n",
    "    spectral_analysis(rt2, 'ts2', freq_list=['H', 'D', 'W'])\n",
    "\n",
    "    rt = pd.concat([rt1, rt2], axis=1)\n",
    "    print(\"Distribution analysis for log returns:\")\n",
    "    analyse_distribution(rt)\n",
    "\n",
    "    print(\"Volatility clustering analysis for rt1:\")\n",
    "    assess_and_plot_volatility_clustering(rt1)\n",
    "\n",
    "    print(\"Volatility clustering analysis for rt2:\")\n",
    "    assess_and_plot_volatility_clustering(rt2)\n",
    "\n",
    "    print(\"Outlier analysis for rt1 and rt2:\")\n",
    "    rt1_d = rt1.resample('H').mean().dropna()\n",
    "    rt2_d = rt2.resample('H').mean().dropna()\n",
    "    analyse_outliers(rt1_d, rt2_d, 'Hourly')\n",
    "    rt1_d = rt1.resample('D').mean().dropna()\n",
    "    rt2_d = rt2.resample('D').mean().dropna()\n",
    "    analyse_outliers(rt1_d, rt2_d, 'Daily')\n",
    "\n",
    "\n",
    "def load_clean_data(filename):\n",
    "    clean_df = pd.read_csv(filename)\n",
    "    clean_df.columns = ['datetime', 'ts1_imputed', 'ts2_imputed']\n",
    "    clean_df.set_index('datetime', inplace=True)\n",
    "    clean_df.index = pd.to_datetime(clean_df.index)\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def calculate_log_returns(df):\n",
    "    rt = np.log(df / df.shift(1))\n",
    "    rt.dropna(inplace=True)\n",
    "    return rt\n",
    "\n",
    "\n",
    "def load_raw_data(filename):\n",
    "    raw_df = pd.read_csv(filename, header=None)\n",
    "    raw_df.columns = ['datetime', 'ts1_raw', 'ts2_raw']\n",
    "    raw_df['datetime'] = pd.to_datetime(\n",
    "        raw_df['datetime'] - 719529, unit='d').round('s')\n",
    "    raw_df.set_index('datetime', inplace=True)\n",
    "    raw_df.dropna(inplace=True)\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def load_data_and_calculate_returns(filename, imputed=False):\n",
    "    if imputed:\n",
    "        df = load_clean_data(filename)\n",
    "        ts_1, ts_2 = df['ts1_imputed'], df['ts2_imputed']\n",
    "        rt = calculate_log_returns(df)\n",
    "        rt_1, rt_2 = rt['ts1_imputed'], rt['ts2_imputed']\n",
    "    else:\n",
    "        df = load_raw_data(filename)\n",
    "        ts_1, ts_2 = df['ts1_raw'], df['ts2_raw']\n",
    "        rt = calculate_log_returns(df)\n",
    "        rt_1, rt_2 = rt['ts1_raw'], rt['ts2_raw']\n",
    "\n",
    "    return ts_1, ts_2, rt_1, rt_2\n",
    "\n",
    "\n",
    "def scale(df):\n",
    "    df_ = df.copy()\n",
    "    for i in df_.columns[0:]:\n",
    "        df_[i] = (df_[i]-df_[i].mean())/df_[i].std()\n",
    "    return df_\n",
    "\n",
    "def main(): \n",
    "    # Load imputed data and calculate returns\n",
    "    ts_1, ts_2, rt_1, rt_2 = load_data_and_calculate_returns(\n",
    "        'interpolate_clean_df.csv', imputed=True)\n",
    "    # Load non-imputed data and calculate returns\n",
    "    ts1_o, ts2_o, rt_1_o, rt_2_o = load_data_and_calculate_returns(\n",
    "        \"Test_data.csv\", imputed=False)\n",
    "\n",
    "    # plot normalised time series\n",
    "    plot_normalised_price(ts1_o, ts2_o)\n",
    "\n",
    "    # non-imputed data\n",
    "    plot_series_and_returns(ts1_o, 'ts1_raw')\n",
    "    plot_series_and_returns(ts2_o, 'ts2_raw')\n",
    "    # imputed data\n",
    "    plot_series_and_returns(ts_1, 'ts1_imputed')\n",
    "    plot_series_and_returns(ts_2, 'ts2_imputed')\n",
    "\n",
    "    # analyse time series\n",
    "    # non-imputed data\n",
    "    analyse_time_series(ts1_o, ts2_o, rt_1_o, rt_2_o)\n",
    "    # imputed data\n",
    "    analyse_time_series(ts_1, ts_2, rt_1, rt_2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seasonality analysis for ts1:\n",
      "Seasonality analysis for ts2:\n",
      "Spectral analysis for rt1:\n",
      "43805\n",
      "Resampled data for frequency: H\n",
      "The input series is empty.\n",
      "1827\n",
      "Resampled data for frequency: D\n",
      "The input series is empty.\n",
      "262\n",
      "Resampled data for frequency: W\n",
      "The input series is empty.\n",
      "Spectral analysis for rt2:\n",
      "43805\n",
      "Resampled data for frequency: H\n",
      "The input series is empty.\n",
      "1827\n",
      "Resampled data for frequency: D\n",
      "The input series is empty.\n",
      "262\n",
      "Resampled data for frequency: W\n",
      "The input series is empty.\n",
      "Distribution analysis for log returns:\n",
      "Data statistics:\n",
      "Mean:\n",
      " ts1_raw    2.018239e-07\n",
      "ts2_raw    8.508173e-07\n",
      "dtype: float64\n",
      "Variance:\n",
      " ts1_raw    3.899748e-09\n",
      "ts2_raw    7.653261e-07\n",
      "dtype: float64\n",
      "Covariance:\n",
      "               ts1_raw       ts2_raw\n",
      "ts1_raw  3.899748e-09 -6.087201e-09\n",
      "ts2_raw -6.087201e-09  7.653261e-07\n",
      "Rolling statistics of ts1_raw:\n",
      "Rolling statistics of ts2_raw:\n",
      "Distribution analysis of ts1_raw:\n",
      "Distribution analysis of ts2_raw:\n",
      "Normality analysis:\n",
      "Kolmogorov-Smirnov test for 'ts1_raw': The distribution is not normal (p-value = 0.0)\n",
      "Kolmogorov-Smirnov test for 'ts2_raw': The distribution is not normal (p-value = 0.0)\n",
      "Skewness for ts1_raw: 2.793381452056887\n",
      "Kurtosis for ts1_raw: 206.37894222338264\n",
      "Skewness for ts2_raw: -0.36962540627444856\n",
      "Kurtosis for ts2_raw: 88.24515205506846\n",
      "Stationarity of ts1_raw:\n",
      "Stationarity of ts2_raw:\n",
      "Volatility clustering analysis for rt1:\n",
      "Volatility clustering analysis for rt2:\n",
      "Outlier analysis for rt1 and rt2:\n",
      "Number of outliers in volatility of ts1: 17\n",
      "Number of outliers in volatility of ts2: 26\n",
      "Intersection times:\n",
      "DatetimeIndex(['2008-10-13 06:00:00', '2008-11-21 17:00:00'], dtype='datetime64[ns]', name='datetime', freq=None)\n",
      "Series 1 non-NaN count: 25941\n",
      "Series 2 non-NaN count: 25941\n",
      "Series 1 outliers count: 17\n",
      "Series 2 outliers count: 26\n",
      "Intersection of outlier series count: 2\n",
      "Number of outliers in volatility of ts1: 9\n",
      "Number of outliers in volatility of ts2: 12\n",
      "Intersection times:\n",
      "DatetimeIndex(['2008-11-16', '2010-06-20'], dtype='datetime64[ns]', name='datetime', freq=None)\n",
      "Series 1 non-NaN count: 1532\n",
      "Series 2 non-NaN count: 1532\n",
      "Series 1 outliers count: 9\n",
      "Series 2 outliers count: 12\n",
      "Intersection of outlier series count: 2\n",
      "Seasonality analysis for ts1:\n",
      "Seasonality analysis for ts2:\n",
      "Spectral analysis for rt1:\n",
      "43849\n",
      "Resampled data for frequency: H\n",
      "1      3.463700e-06\n",
      "2      2.097689e-05\n",
      "3      8.110077e-05\n",
      "4      7.486018e-05\n",
      "5      2.058402e-07\n",
      "           ...     \n",
      "250    3.440942e-32\n",
      "251    3.904997e-32\n",
      "252    5.764869e-32\n",
      "253    6.871217e-32\n",
      "254    3.945827e-33\n",
      "Name: lb_pvalue, Length: 254, dtype: float64\n",
      "1828\n",
      "Resampled data for frequency: D\n",
      "1     3.769456e-01\n",
      "2     1.205187e-06\n",
      "3     2.519532e-06\n",
      "4     7.107001e-06\n",
      "5     1.936107e-05\n",
      "6     2.714210e-05\n",
      "7     1.465314e-06\n",
      "8     1.266897e-10\n",
      "9     1.767474e-10\n",
      "10    1.329070e-10\n",
      "11    3.527192e-10\n",
      "12    5.472772e-11\n",
      "13    3.515570e-11\n",
      "14    8.412726e-11\n",
      "15    1.576976e-10\n",
      "16    9.560455e-11\n",
      "17    1.773584e-10\n",
      "18    5.637724e-11\n",
      "19    1.253886e-10\n",
      "20    1.225985e-10\n",
      "21    6.568505e-12\n",
      "22    1.173425e-11\n",
      "23    2.398496e-11\n",
      "24    4.958875e-11\n",
      "25    3.549042e-11\n",
      "26    6.432422e-11\n",
      "27    1.931598e-11\n",
      "28    3.140544e-12\n",
      "29    1.245454e-12\n",
      "Name: lb_pvalue, dtype: float64\n",
      "262\n",
      "Resampled data for frequency: W\n",
      "The input series is empty.\n",
      "Spectral analysis for rt2:\n",
      "43849\n",
      "Resampled data for frequency: H\n",
      "1        3.127206e-05\n",
      "2        3.096851e-07\n",
      "3        8.829427e-07\n",
      "4        2.324004e-06\n",
      "5        7.141294e-06\n",
      "            ...      \n",
      "1047    4.448327e-207\n",
      "1048    3.493239e-207\n",
      "1049    8.124560e-209\n",
      "1050    6.589835e-209\n",
      "1051    4.720592e-209\n",
      "Name: lb_pvalue, Length: 1051, dtype: float64\n",
      "1828\n",
      "Resampled data for frequency: D\n",
      "1     6.867008e-01\n",
      "2     7.833950e-01\n",
      "3     9.188539e-01\n",
      "4     6.555660e-01\n",
      "5     2.655402e-01\n",
      "6     4.217002e-02\n",
      "7     4.782667e-02\n",
      "8     6.602140e-02\n",
      "9     5.815218e-02\n",
      "10    8.558659e-02\n",
      "11    7.485052e-02\n",
      "12    1.019400e-01\n",
      "13    4.920694e-02\n",
      "14    4.270677e-02\n",
      "15    9.067718e-03\n",
      "16    1.629996e-04\n",
      "17    2.840679e-04\n",
      "18    1.967853e-04\n",
      "19    5.571218e-05\n",
      "20    3.804100e-05\n",
      "21    6.444063e-05\n",
      "22    9.463208e-05\n",
      "23    1.181186e-04\n",
      "24    1.682103e-04\n",
      "25    2.134949e-04\n",
      "26    3.220765e-04\n",
      "27    4.086990e-04\n",
      "28    5.229210e-04\n",
      "29    6.095417e-04\n",
      "30    8.905370e-04\n",
      "31    1.276720e-03\n",
      "32    1.837420e-03\n",
      "33    4.163968e-04\n",
      "34    5.746701e-04\n",
      "35    8.311251e-04\n",
      "36    1.113223e-03\n",
      "37    1.078051e-04\n",
      "38    9.600622e-05\n",
      "39    1.114261e-04\n",
      "40    5.763196e-05\n",
      "41    2.182342e-05\n",
      "42    1.967795e-05\n",
      "43    5.084673e-06\n",
      "44    7.231131e-06\n",
      "45    9.952616e-07\n",
      "46    5.233184e-07\n",
      "Name: lb_pvalue, dtype: float64\n",
      "262\n",
      "Resampled data for frequency: W\n",
      "The input series is empty.\n",
      "Distribution analysis for log returns:\n",
      "Data statistics:\n",
      "Mean:\n",
      " ts1_imputed    3.087256e-08\n",
      "ts2_imputed    1.311734e-07\n",
      "dtype: float64\n",
      "Variance:\n",
      " ts1_imputed    4.007718e-10\n",
      "ts2_imputed    1.258436e-07\n",
      "dtype: float64\n",
      "Covariance:\n",
      "               ts1_imputed   ts2_imputed\n",
      "ts1_imputed  4.007718e-10 -4.528894e-10\n",
      "ts2_imputed -4.528894e-10  1.258436e-07\n",
      "Rolling statistics of ts1_imputed:\n",
      "Rolling statistics of ts2_imputed:\n",
      "Distribution analysis of ts1_imputed:\n",
      "Distribution analysis of ts2_imputed:\n",
      "Normality analysis:\n",
      "Kolmogorov-Smirnov test for 'ts1_imputed': The distribution is not normal (p-value = 0.0)\n",
      "Kolmogorov-Smirnov test for 'ts2_imputed': The distribution is not normal (p-value = 0.0)\n",
      "Skewness for ts1_imputed: 11.182719731307047\n",
      "Kurtosis for ts1_imputed: 2558.786844858847\n",
      "Skewness for ts2_imputed: 1.2671179794686382\n",
      "Kurtosis for ts2_imputed: 232.0938218353917\n",
      "Stationarity of ts1_imputed:\n",
      "Stationarity of ts2_imputed:\n",
      "Volatility clustering analysis for rt1:\n",
      "Volatility clustering analysis for rt2:\n",
      "Outlier analysis for rt1 and rt2:\n",
      "Number of outliers in volatility of ts1: 10\n",
      "Number of outliers in volatility of ts2: 21\n",
      "Intersection times:\n",
      "DatetimeIndex([], dtype='datetime64[ns]', name='datetime', freq=None)\n",
      "Series 1 non-NaN count: 43849\n",
      "Series 2 non-NaN count: 43849\n",
      "Series 1 outliers count: 10\n",
      "Series 2 outliers count: 21\n",
      "Intersection of outlier series count: 0\n",
      "Number of outliers in volatility of ts1: 12\n",
      "Number of outliers in volatility of ts2: 11\n",
      "Intersection times:\n",
      "DatetimeIndex(['2008-10-15'], dtype='datetime64[ns]', name='datetime', freq=None)\n",
      "Series 1 non-NaN count: 1828\n",
      "Series 2 non-NaN count: 1828\n",
      "Series 1 outliers count: 12\n",
      "Series 2 outliers count: 11\n",
      "Intersection of outlier series count: 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
