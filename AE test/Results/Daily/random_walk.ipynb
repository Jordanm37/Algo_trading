{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime as dt\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    mean_absolute_percentage_error, \n",
    "    r2_score, \n",
    "    mean_squared_log_error\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, \n",
    "    GradientBoostingRegressor, \n",
    "    ExtraTreesRegressor, \n",
    "    AdaBoostRegressor\n",
    ")\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, DotProduct, WhiteKernel\n",
    "import xgboost as xgb\n",
    "\n",
    "# Time series libraries\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey, acorr_ljungbox, het_white\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAXResults\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from arch import arch_model\n",
    "import pmdarima as pm\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.model_selection import ForecastingGridSearchCV, SlidingWindowSplitter\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey, acorr_ljungbox, het_white\n",
    "from scipy.stats import jarque_bera\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from scipy.stats import probplot, moment\n",
    "\n",
    "# LightGBM library\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Set options\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "    }\n",
    "\n",
    "# univaritae\n",
    "## Simple models for benchmarking\n",
    "def random_walk_forecast(start_point, forecast_length, return_mean, return_std, index):\n",
    "    prediction = [start_point]\n",
    "\n",
    "    for i in range(1, forecast_length):\n",
    "        next_value = prediction[-1] * (1 + random.normalvariate(mu=return_mean, sigma=return_std))\n",
    "        prediction.append(next_value)\n",
    "\n",
    "    return pd.Series(prediction, index=index)\n",
    "\n",
    "def calculate_benchmark_errors(series):\n",
    "    mean_returns, std_returns = calculate_returns_stats(series)\n",
    "    forecast_length = len(series)\n",
    "    start_point = series[0]\n",
    "    predictions = random_walk_forecast(start_point, forecast_length, mean_returns, std_returns, series.index)\n",
    "    # print(predictions)\n",
    "    evaluation_result = evaluate(series, predictions)\n",
    "    # Return a dictionary containing the evaluation results for each baseline\n",
    "    return {\n",
    "        'Benchmark MAE': evaluation_result['mae'],\n",
    "        'Benchmark RMSE': evaluation_result['rmse'],\n",
    "    }\n",
    "\n",
    "def calculate_returns_stats(time_series):\n",
    "    # Calculate the returns\n",
    "    returns = time_series / time_series.shift(1) - 1\n",
    "    returns = returns.dropna()\n",
    "\n",
    "    # Calculate mean returns and standard deviation of returns\n",
    "    mean_returns = returns.mean()\n",
    "    std_returns = returns.std()\n",
    "\n",
    "    return mean_returns, std_returns\n",
    "\n",
    "# multivariate\n",
    "def random_walk_forecast_multivariate(start_points, forecast_length, return_means, return_stds, index):\n",
    "    num_variables = len(start_points)\n",
    "    predictions = [start_points]\n",
    "\n",
    "    for _ in range(1, forecast_length):\n",
    "        next_values = [predictions[-1][i] * (1 + random.normalvariate(mu=return_means[i], sigma=return_stds[i])) for i in range(num_variables)]\n",
    "        predictions.append(next_values)\n",
    "\n",
    "    # Transpose the list of predictions and convert each variable's predictions to a pandas Series\n",
    "    predictions_transposed = list(map(list, zip(*predictions)))\n",
    "    prediction_series = [pd.Series(pred, index=index) for pred in predictions_transposed]\n",
    "\n",
    "    return prediction_series\n",
    "    \n",
    "def monte_carlo_simulation_univariate(series, n_trials=10000):\n",
    "    # Initialize a dictionary to store the accumulated evaluation results\n",
    "    accumulated_results = {\n",
    "        'Benchmark MAE': 0,\n",
    "        'Benchmark RMSE': 0,\n",
    "    }\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        # Calculate benchmark errors for this trial\n",
    "        trial_results = calculate_benchmark_errors(series)\n",
    "\n",
    "        # Accumulate the results of this trial\n",
    "        for key in accumulated_results.keys():\n",
    "            accumulated_results[key] += trial_results[key]\n",
    "\n",
    "    # Average the accumulated results over the number of trials\n",
    "    for key in accumulated_results.keys():\n",
    "        accumulated_results[key] /= n_trials\n",
    "\n",
    "    return accumulated_results\n",
    "\n",
    "def calculate_benchmark_errors_mult(series1, series2):\n",
    "    mean_returns1, std_returns1 = calculate_returns_stats(series1)\n",
    "    mean_returns2, std_returns2 = calculate_returns_stats(series2)\n",
    "    \n",
    "    forecast_length = len(series1)\n",
    "    start_points = [series1[0], series2[0]]\n",
    "    return_means = [mean_returns1, mean_returns2]\n",
    "    return_stds = [std_returns1, std_returns2]\n",
    "    \n",
    "    predictions = random_walk_forecast_multivariate(start_points, forecast_length, return_means, return_stds, series1.index)\n",
    "    \n",
    "    evaluation_result1 = evaluate(series1, predictions[0])\n",
    "    evaluation_result2 = evaluate(series2, predictions[1])\n",
    "    # plot_time_series(series1, predictions[0])\n",
    "    # plot_time_series(series2, predictions[1])\n",
    "    \n",
    "    return {\n",
    "        'Series1 Benchmark MAE': evaluation_result1['mae'],\n",
    "        'Series1 Benchmark RMSE': evaluation_result1['rmse'],\n",
    "        'Series2 Benchmark MAE': evaluation_result2['mae'],\n",
    "        'Series2 Benchmark RMSE': evaluation_result2['rmse'],\n",
    "    }\n",
    "    \n",
    "def monte_carlo_simulation_multivariate(series1, series2, n_trials=10000):\n",
    "    # Initialize a dictionary to store the accumulated evaluation results\n",
    "    accumulated_results = {\n",
    "        'Series1 Benchmark MAE': 0,\n",
    "        'Series1 Benchmark RMSE': 0,\n",
    "        'Series2 Benchmark MAE': 0,\n",
    "        'Series2 Benchmark RMSE': 0,\n",
    "    }\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        # Calculate benchmark errors for this trial\n",
    "        trial_results = calculate_benchmark_errors_mult(series1, series2)\n",
    "\n",
    "        # Accumulate the results of this trial\n",
    "        for key in accumulated_results.keys():\n",
    "            accumulated_results[key] += trial_results[key]\n",
    "\n",
    "    # Average the accumulated results over the number of trials\n",
    "    for key in accumulated_results.keys():\n",
    "        accumulated_results[key] /= n_trials\n",
    "\n",
    "    return accumulated_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(filename):\n",
    "    raw_df = pd.read_csv(filename, header=None)\n",
    "    raw_df.columns = ['datetime', 'ts1', 'ts2']\n",
    "    raw_df['datetime'] = pd.to_datetime(raw_df['datetime'] - 719529, unit='d').round('s')\n",
    "    raw_df.set_index('datetime', inplace=True)\n",
    "    raw_df.dropna(inplace=True)\n",
    "    return raw_df\n",
    "\n",
    "def calculate_log_returns(df):\n",
    "    rt = np.log(df / df.shift(1))\n",
    "    rt.dropna(inplace=True)\n",
    "    return rt\n",
    "\n",
    "def load_clean_data(filename):\n",
    "    clean_df = pd.read_csv(filename)\n",
    "    clean_df.columns = ['datetime', 'ts1', 'ts2']\n",
    "    clean_df.set_index('datetime', inplace=True)\n",
    "    clean_df.index = pd.to_datetime(clean_df.index)\n",
    "    return clean_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_series(series, split_date):\n",
    "    series.index = pd.to_datetime(series.index)\n",
    "    split_date = pd.Timestamp(split_date)\n",
    "    \n",
    "    before_split = series.loc[series.index <= split_date]\n",
    "    after_split = series.loc[series.index > split_date]\n",
    "    \n",
    "    return before_split, after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load raw data\n",
    "df = load_raw_data(\"Test_data.csv\")\n",
    "#calcualte log returns\n",
    "raw_rt = calculate_log_returns(df)\n",
    "raw_rt = raw_rt.resample('D').mean().dropna()\n",
    "train_raw, test_raw = split_series(raw_df, '2011-12-31')\n",
    "\n",
    "\n",
    "# df = load_clean_data('imputed_df.csv')\n",
    "# #calcualte log returns\n",
    "# imp_rt = calculate_log_returns(df)\n",
    "# imp_rt_daily = imp_rt.resample('D').mean()\n",
    "\n",
    "# # prerpare train and test experiments\n",
    "# # train_imp, test_imp = split_series(imp_rt_daily, '2011-12-31')\n",
    "# train_raw, test_raw = split_series(raw_rt_daily, '2011-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate\n",
      "\n",
      "H1\n",
      "\n",
      "Train\n",
      "{'Benchmark MAE': 1.425311743138642, 'Benchmark RMSE': 1.6848674310245457}\n",
      "\n",
      "Test\n",
      "{'Benchmark MAE': 1.4344625599349239, 'Benchmark RMSE': 1.696045764746107}\n",
      "\n",
      "H2\n",
      "\n",
      "Train\n",
      "{'Benchmark MAE': 0.04702588537341932, 'Benchmark RMSE': 0.055172784438008944}\n",
      "\n",
      "Test\n",
      "{'Benchmark MAE': 0.04752462822795691, 'Benchmark RMSE': 0.05565652110530692}\n",
      "Multivariate\n",
      "\n",
      "H3/H4\n",
      "\n",
      "Train\n",
      "{'Series1 Benchmark MAE': 1.4333394867764166, 'Series1 Benchmark RMSE': 1.69375050541172, 'Series2 Benchmark MAE': 0.18362620521903997, 'Series2 Benchmark RMSE': 0.2174841209653639}\n",
      "\n",
      "Test\n",
      "{'Series1 Benchmark MAE': 0.445771108719327, 'Series1 Benchmark RMSE': 0.5211173588374408, 'Series2 Benchmark MAE': 0.04686230573111296, 'Series2 Benchmark RMSE': 0.05491780070621427}\n"
     ]
    }
   ],
   "source": [
    "def calculate_benchmark_univariate(series, var, n_trials):\n",
    "    print('\\nTrain')\n",
    "    results = monte_carlo_simulation_univariate(series[var], n_trials=10000)\n",
    "    print(results)\n",
    "    print('\\nTest')\n",
    "    results = monte_carlo_simulation_univariate(series[var], n_trials=10000)\n",
    "    print(results)    \n",
    "    \n",
    "def calulate_benchmark_multivariate(series1, series2, var1, var2, n_trials):\n",
    "    print('\\nTrain')\n",
    "    results = monte_carlo_simulation_multivariate(series1[var1], series1[var2], n_trials=10000)\n",
    "    print(results)\n",
    "    print('\\nTest')\n",
    "    results = monte_carlo_simulation_multivariate(series2[var1], series2[var2], n_trials=10000)     \n",
    "    print(results)\n",
    "    \n",
    "    \n",
    "n_trials = 10000\n",
    "print('Univariate')\n",
    "print('\\nH1')\n",
    "calculate_benchmark_univariate(train_raw, 'ts1', n_trials=n_trials)\n",
    "print('\\nH2')\n",
    "calculate_benchmark_univariate(test_raw, 'ts2', n_trials=n_trials)\n",
    "print('Multivariate')\n",
    "print('\\nH3/H4')\n",
    "calulate_benchmark_multivariate(train_raw, test_raw, 'ts1', 'ts2', n_trials=n_trials)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Univariate')\n",
    "# H1\n",
    "print('H1')\n",
    "print('train')\n",
    "average_results = monte_carlo_simulation_univariate(train_raw['ts1'], n_trials=100)\n",
    "print(average_results)\n",
    "print('test')\n",
    "average_results = monte_carlo_simulation_univariate(test_raw['ts1'], n_trials=100)\n",
    "print(average_results)\n",
    "# H2\n",
    "print('\\nH2')\n",
    "print('train')\n",
    "average_results = monte_carlo_simulation_univariate(train_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n",
    "print('test')\n",
    "average_results = monte_carlo_simulation_univariate(test_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n",
    "# H3/h4\n",
    "print('\\nH3/H4')\n",
    "print('train')\n",
    "average_results = monte_carlo_simulation_multivariate(train_raw['ts1'], train_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n",
    "print('test')\n",
    "average_results = monte_carlo_simulation_multivariate(test_raw['ts1'], test_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_raw_data(\"Test_data.csv\")\n",
    "raw_df = df.resample('D').mean().dropna()\n",
    "raw_ts_1_daily = raw_df['ts1'].dropna()\n",
    "raw_ts_2_daily = raw_df['ts2'].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Monte Carlo simulation on your time series data\n",
    "print('Univariate')\n",
    "average_results = monte_carlo_simulation(raw_ts_1_daily, n_trials=1000)\n",
    "print(average_results)\n",
    "average_results = monte_carlo_simulation(raw_ts_2_daily, n_trials=1000)\n",
    "print(average_results)\n",
    "print('Multivariate')\n",
    "# Run the Monte Carlo simulation on your time series data\n",
    "average_results = monte_carlo_simulation_mult(raw_ts_1_daily, raw_ts_2_daily)\n",
    "print(average_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "    }\n",
    "\n",
    "def calculate_log_returns(series):\n",
    "    return np.log(series / series.shift(1)).dropna()\n",
    "\n",
    "def calculate_return_statistics(time_series):\n",
    "    mean_returns = time_series.mean()\n",
    "    std_returns = time_series.std()\n",
    "\n",
    "    return mean_returns, std_returns\n",
    "\n",
    "def random_walk_forecast(start_point, forecast_length, return_mean, return_std, index):\n",
    "    prediction = [start_point]\n",
    "\n",
    "    for _ in range(1, forecast_length):\n",
    "        next_value = prediction[-1] * (1 + random.normalvariate(mu=return_mean, sigma=return_std))\n",
    "        prediction.append(next_value)\n",
    "\n",
    "    return pd.Series(prediction, index=index)\n",
    "\n",
    "def evaluate_univariate_random_walk(series, n_trials=10000):\n",
    "    accumulated_results = {\n",
    "        'Benchmark MAE': 0,\n",
    "        'Benchmark RMSE': 0,\n",
    "    }\n",
    "\n",
    "    mean_returns, std_returns = calculate_return_statistics(series)\n",
    "    forecast_length = len(series)\n",
    "    start_point = series[0]\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        predictions = random_walk_forecast(start_point, forecast_length, mean_returns, std_returns, series.index)\n",
    "        trial_results = evaluate(series, predictions)\n",
    "\n",
    "        for key in accumulated_results.keys():\n",
    "            accumulated_results[key] += trial_results[key]\n",
    "\n",
    "    for key in accumulated_results.keys():\n",
    "        accumulated_results[key] /= n_trials\n",
    "\n",
    "    return accumulated_results\n",
    "\n",
    "def evaluate_multivariate_random_walk(series1, series2, n_trials=10000):\n",
    "    accumulated_results = {\n",
    "        'Series1 Benchmark MAE': 0,\n",
    "        'Series1 Benchmark RMSE': 0,\n",
    "        'Series2 Benchmark MAE': 0,\n",
    "        'Series2 Benchmark RMSE': 0,\n",
    "    }\n",
    "\n",
    "    mean_returns1, std_returns1 = calculate_return_statistics(series1)\n",
    "    mean_returns2, std_returns2 = calculate_return_statistics(series2)\n",
    "    forecast_length = len(series1)\n",
    "    start_points = [series1[0], series2[0]]\n",
    "    return_means = [mean_returns1, mean_returns2]\n",
    "    return_stds = [std_returns1, std_returns2]\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        prediction_series = random_walk_forecast_multivariate(start_points, forecast_length, return_means, return_stds, series1.index)\n",
    "        evaluation_result1 = evaluate(series1, prediction_series[0])\n",
    "        evaluation_result2 = evaluate(series2, prediction_series[1])\n",
    "\n",
    "        for key in accumulated_results.keys():\n",
    "            if \"Series1\" in key:\n",
    "                accumulated_results[key] += evaluation_result1[key.replace(\"Series1 \", \"\")]\n",
    "            else:\n",
    "                accumulated_results[key] += evaluation_result2[key.replace(\"Series2 \", \"\")]\n",
    "\n",
    "    for key in accumulated_results.keys():\n",
    "        accumulated_results[key] /= n_trials\n",
    "\n",
    "    return accumulated_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Univariate\n",
      "H1\n",
      "train\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mH1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m average_results \u001b[39m=\u001b[39m evaluate_univariate_random_walk(train_raw[\u001b[39m'\u001b[39;49m\u001b[39mts1\u001b[39;49m\u001b[39m'\u001b[39;49m], n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(average_results)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 41\u001b[0m, in \u001b[0;36mevaluate_univariate_random_walk\u001b[1;34m(series, n_trials)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_trials):\n\u001b[0;32m     40\u001b[0m     predictions \u001b[39m=\u001b[39m random_walk_forecast(start_point, forecast_length, mean_returns, std_returns, series\u001b[39m.\u001b[39mindex)\n\u001b[1;32m---> 41\u001b[0m     trial_results \u001b[39m=\u001b[39m evaluate(series, predictions)\n\u001b[0;32m     43\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m accumulated_results\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     44\u001b[0m         accumulated_results[key] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m trial_results[key]\n",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m(y_true, y_pred):\n\u001b[1;32m----> 2\u001b[0m     mae \u001b[39m=\u001b[39m mean_absolute_error(y_true, y_pred)\n\u001b[0;32m      3\u001b[0m     mse \u001b[39m=\u001b[39m mean_squared_error(y_true, y_pred)\n\u001b[0;32m      4\u001b[0m     rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(mse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:196\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_absolute_error\u001b[39m(\n\u001b[0;32m    142\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m ):\n\u001b[0;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m    0.85...\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    197\u001b[0m         y_true, y_pred, multioutput\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    200\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39mabs(y_pred \u001b[39m-\u001b[39m y_true), weights\u001b[39m=\u001b[39msample_weight, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m    101\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m--> 102\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    104\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    105\u001b[0m     y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "print('Univariate')\n",
    "# H1\n",
    "print('H1')\n",
    "print('train')\n",
    "average_results = evaluate_univariate_random_walk(train_raw['ts1'], n_trials=100)\n",
    "print(average_results)\n",
    "print('test')\n",
    "average_results = evaluate_univariate_random_walk(test_raw['ts1'], n_trials=100)\n",
    "print(average_results)\n",
    "# H2\n",
    "print('\\nH2')\n",
    "print('train')\n",
    "average_results = evaluate_univariate_random_walk(train_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n",
    "print('test')\n",
    "average_results = evaluate_univariate_random_walk(test_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n",
    "# H3/h4\n",
    "print('\\nH3/H4')\n",
    "print('train')\n",
    "average_results = evaluate_multivariate_random_walk(train_raw['ts1'], train_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n",
    "print('test')\n",
    "average_results = evaluate_multivariate_random_walk(test_raw['ts1'], test_raw['ts2'], n_trials=100)\n",
    "print(average_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
