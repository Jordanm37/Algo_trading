{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, DotProduct, WhiteKernel\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "#Libraries for Deep Learning Models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import LSTM\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "#Libraries for Statistical Models\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey, acorr_ljungbox, het_white\n",
    "from scipy.stats import jarque_bera\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from scipy.stats import probplot, moment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date tiem features\n",
    "def add_datetime_features(df):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Extract year, month, and day information\n",
    "    df_copy['year'] = df_copy.index.year\n",
    "    df_copy['month'] = df_copy.index.month\n",
    "    df_copy['day'] = df_copy.index.day\n",
    "    df_copy['weekday'] = df_copy.index.weekday\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def generate_lagged_features(df, var, max_lag):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    ts_data = df.copy()\n",
    "    for t in range(1, max_lag + 1):\n",
    "        ts_data[var + '_lag' + str(t)] = ts_data[var].shift(t, freq='1D')\n",
    "\n",
    "    ts_data.dropna(inplace=True)\n",
    "\n",
    "    return ts_data\n",
    "\n",
    "def prepare_time_series_data(df, var, max_lag):\n",
    "    # Generate lagged features\n",
    "    lagged_data = generate_lagged_features(df, var, max_lag)\n",
    "    \n",
    "    # Add datetime features\n",
    "    transformed_data = add_datetime_features(lagged_data)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def extract_column(df, column_name):\n",
    "    extracted_column = df[column_name]\n",
    "    remaining_df = df.drop(column_name, axis=1)\n",
    "    return remaining_df, extracted_column\n",
    "\n",
    "# def scale_data(train_data, test_data):\n",
    "#     scaler = MinMaxScaler()\n",
    "#     train_scaled = scaler.fit_transform(train_data)\n",
    "#     test_scaled = scaler.transform(test_data)\n",
    "#     return train_scaled, test_scaled, scaler\n",
    "\n",
    "def scale_data(train_data, test_data, target_column):\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_Y = MinMaxScaler()\n",
    "    \n",
    "    train_X = train_data.drop(target_column, axis=1)\n",
    "    train_Y = train_data[[target_column]]\n",
    "    \n",
    "    test_X = test_data.drop(target_column, axis=1)\n",
    "    test_Y = test_data[[target_column]]\n",
    "    \n",
    "    train_X_scaled = scaler_X.fit_transform(train_X)\n",
    "    train_Y_scaled = scaler_Y.fit_transform(train_Y)\n",
    "    \n",
    "    test_X_scaled = scaler_X.transform(test_X)\n",
    "    test_Y_scaled = scaler_Y.transform(test_Y)\n",
    "    \n",
    "    train_data_scaled = np.concatenate((train_X_scaled, train_Y_scaled), axis=1)\n",
    "    test_data_scaled = np.concatenate((test_X_scaled, test_Y_scaled), axis=1)\n",
    "    \n",
    "    return train_data_scaled, test_data_scaled, scaler_X, scaler_Y\n",
    "\n",
    "\n",
    "def unscale_data(pred, actual, scaler):\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    actual = scaler.inverse_transform(actual)\n",
    "    return pred, actual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_d = pd.read_csv('rt_daily.csv')\n",
    "rt_d.set_index('datetime', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms\n",
    "rt_d.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(6,6))\n",
    "plt.show()\n",
    "# density\n",
    "rt_d.plot(kind='density', subplots=True, layout=(4,4), sharex=False, legend=True, fontsize=1, figsize=(6,6))\n",
    "plt.show()\n",
    "#Box and Whisker Plots\n",
    "rt_d.plot(kind='box', subplots=True, layout=(4,4), sharex=False, sharey=False, figsize=(6,6))\n",
    "plt.show()\n",
    "\n",
    "rt_transformed = prepare_time_series_data(rt_d, 'ts1', 2)\n",
    "rt_transformed.boxplot(figsize = (20, 5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "# Create a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data (ignoring missing values)\n",
    "minmax_scaler.fit(rt_transformed)\n",
    "\n",
    "# Convert the scaled data back to a Pandas dataframe\n",
    "rt_transformed = pd.DataFrame(minmax_scaler.transform(rt_transformed), columns=rt_transformed.columns)\n",
    "rt_transformed.boxplot(figsize = (20, 5))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Prepare your data (use the prepare_time_series_data function)\n",
    "rt_transformed = prepare_time_series_data(rt_d, 'ts1', 2)\n",
    "\n",
    "# Separate the target variable (Y) and the features (X)\n",
    "Y = rt_transformed['ts1']\n",
    "X = rt_transformed.loc[:, rt_transformed.columns != 'ts1']\n",
    "\n",
    "# Create a SelectKBest object with a scoring function (e.g., f_regression)\n",
    "bestfeatures = SelectKBest(score_func=f_regression, k='all')\n",
    "\n",
    "# Fit the SelectKBest object to the data\n",
    "fit = bestfeatures.fit(X, Y)\n",
    "\n",
    "# Create DataFrames for feature scores and feature names\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "# Concatenate the DataFrames and name the columns\n",
    "feature_scores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "feature_scores.columns = ['Feature', 'Score']\n",
    "\n",
    "# Sort the features by their scores in descending order\n",
    "sorted_feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "\n",
    "print(sorted_feature_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate ML\n",
    "-   Grid search cv\n",
    "-   best model mean and std\n",
    "-   train on all training\n",
    "-   predict on train\n",
    "-   plot resid + squared resid\n",
    "-   predict on test\n",
    "-   plot resid + squared residual\n",
    "-   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # msle = mean_squared_log_error(y_true, y_pred)\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "    }\n",
    "\n",
    "def perform_grid_search(model, param_grid, X_train, Y_train, cv):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=3,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    mean_score = -np.mean(cv_results['mean_test_score'])\n",
    "    std_score = np.std(cv_results['mean_test_score'])\n",
    "    \n",
    "    # Take the square root of the mean_score and std_score\n",
    "    rmse_mean = np.sqrt(mean_score)\n",
    "    rmse_std = np.sqrt(std_score)\n",
    "    \n",
    "    return best_model, rmse_mean, rmse_std\n",
    "\n",
    "def train_best_model(best_model, X_train, Y_train):\n",
    "    best_model.fit(X_train, Y_train)\n",
    "    return best_model\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def reshape_and_unscale_predictions(preds, actuals, scaler_Y):\n",
    "    # Reshape predictions and actual values for unscaling\n",
    "    preds_reshaped = preds.reshape(-1, 1)\n",
    "    actuals_reshaped = actuals.values.reshape(-1, 1)\n",
    "\n",
    "    # Unscale the predictions and actual values\n",
    "    unscaled_preds, unscaled_actuals = unscale_data(preds_reshaped, actuals_reshaped, scaler_Y)\n",
    "\n",
    "    # Convert the unscaled predictions and actuals back to pandas Series\n",
    "    unscaled_preds_series = pd.Series(unscaled_preds.squeeze(), index=actuals.index)\n",
    "    unscaled_actuals_series = pd.Series(unscaled_actuals.squeeze(), index=actuals.index)\n",
    "\n",
    "    return unscaled_preds_series, unscaled_actuals_series\n",
    "\n",
    "    \n",
    "def create_directory_if_not_exists(directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots_for_experiment(experiment, hypothesis, model_name, train_or_test, plot_type):\n",
    "    # Create the necessary directories\n",
    "    experiment_path = os.path.join(experiment, hypothesis, train_or_test)\n",
    "    create_directory_if_not_exists(experiment_path)\n",
    "    \n",
    "    # Return the save path\n",
    "    save_path = os.path.join(experiment_path, f\"{model_name}_{plot_type}.png\")\n",
    "    return save_path\n",
    "\n",
    "def model_pipeline(model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, hypothesis, model_name,experiment, show_plot=False):\n",
    "    # Perform grid search\n",
    "    best_model, rmean_error_train, std_error = perform_grid_search(model, param_grid, X_train, Y_train, tscv)\n",
    "    \n",
    "    print(\"Best model:\", best_model)\n",
    "    print(\"Mean error:\", rmean_error_train)\n",
    "    print(\"Standard error:\", std_error)\n",
    "    \n",
    "    # Train best model on all training data\n",
    "    best_model_trained = train_best_model(best_model, X_train, Y_train)\n",
    "    \n",
    "    # Predict on train set\n",
    "    train_preds = predict(best_model_trained, X_train)\n",
    "    \n",
    "    # Unscale and plot predictions vs actuals for train set\n",
    "    unscaled_preds_series, unscaled_actuals_series = reshape_and_unscale_predictions(train_preds, Y_train, scaler_Y)\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"train\", \"predictions_vs_actuals\")\n",
    "    plot_series(unscaled_actuals_series, unscaled_preds_series, title=\"Unscaled Predictions vs Actual Training Data\", save_path=save_path, show_plot=show_plot)\n",
    "\n",
    "    # Perform residual analysis for train set\n",
    "    residuals_train = unscaled_actuals_series - unscaled_preds_series\n",
    "    print(\"Residual analysis for train set:\")\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"train\", \"correlogram\")\n",
    "    plot_correlogram(residuals_train, save_path=save_path, show_plot=show_plot)\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"train\", \"homoskedasticity_plot\")\n",
    "    residual_analysis(residuals_train, unscaled_preds_series, save_path=save_path)\n",
    "    print(\"Residuals quared\\n\")\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"train\", \"correlogram_squared\")\n",
    "    plot_correlogram(residuals_train.pow(2), save_path=save_path, show_plot=False)\n",
    "    residual_analysis(residuals_train.pow(2), unscaled_preds_series.pow(2), save_path=None)\n",
    "    \n",
    "    # Predict on test set\n",
    "    test_preds = predict(best_model_trained, X_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(Y_test, test_preds))\n",
    "    print(\"RMSE:\", rmse_test)\n",
    "    \n",
    "    # Unscale and plot predictions vs actuals for test set\n",
    "    unscaled_preds_series, unscaled_actuals_series = reshape_and_unscale_predictions(test_preds, Y_test, scaler_Y)\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"test\", \"predictions_vs_actuals\")\n",
    "    plot_series(unscaled_actuals_series, unscaled_preds_series, title=\"Unscaled Predictions vs Actual Test Data\", save_path=save_path, show_plot=show_plot)\n",
    "\n",
    "    # Perform residual analysis for test set\n",
    "    residuals_test = unscaled_actuals_series - unscaled_preds_series\n",
    "    print(\"Residual analysis for test set:\")\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"test\", \"correlogram\")\n",
    "    plot_correlogram(residuals_test, save_path=save_path, show_plot=show_plot)\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"test\", \"homoskedasticity_plot\")\n",
    "    residual_analysis(residuals_test, unscaled_preds_series, save_path=save_path)\n",
    "    print(\"Residuals quared\\n\")\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, model_name, \"test\", \"correlogram_squared\")\n",
    "    plot_correlogram(residuals_test.pow(2), title='Residuals Squared', save_path=save_path, show_plot=False)\n",
    "    residual_analysis(residuals_test.pow(2), unscaled_preds_series.pow(2), save_path=None)\n",
    "\n",
    "    return best_model_trained , rmean_error_train, rmse_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(series1, series2, label1=\"Actual\", label2=\"Predicted\", title=\"Unscaled Predictions vs Actual Data\", save_path=None, show_plot=False):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(series1.index, series1, label=label1)\n",
    "    ax.plot(series2.index, series2, label=label2, linestyle=\"--\")\n",
    "    ax.legend()\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(title)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    # if show_plot:\n",
    "    #     plt.show()\n",
    "\n",
    "def plot_correlogram(x, lags=None, title=None,save_path=None, show_plot=False):\n",
    "    lags = min(10, int(len(x)/5)) if lags is None else lags\n",
    "    x = x + np.random.normal(0, 1e-10, len(x)) ## Add noise to avoid non-invertibility\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))\n",
    "    \n",
    "    # Residuals plot\n",
    "    x.plot(ax=axes[0][0], title='Residuals')\n",
    "    x.rolling(21).mean().plot(ax=axes[0][0], c='k', lw=1)\n",
    "    q_p = np.max(q_stat(acf(x, nlags=lags), len(x))[1])\n",
    "    stats = f'Q-Stat: {np.max(q_p):>8.2f}\\nADF: {adfuller(x)[1]:>11.2f}'\n",
    "    axes[0][0].text(x=.02, y=.85, s=stats, transform=axes[0][0].transAxes)\n",
    "    \n",
    "    # Probability plot\n",
    "    probplot(x, plot=axes[0][1])\n",
    "    mean, var, skew, kurtosis = moment(x, moment=[1, 2, 3, 4])\n",
    "    s = f'Mean: {mean:>12.2f}\\nSD: {np.sqrt(var):>16.2f}\\nSkew: {skew:12.2f}\\nKurtosis:{kurtosis:9.2f}'\n",
    "    axes[0][1].text(x=.02, y=.75, s=s, transform=axes[0][1].transAxes)\n",
    "    \n",
    "    # ACF and PACF plots\n",
    "    plot_acf(x=x, lags=lags, zero=False, ax=axes[1][0])\n",
    "    plot_pacf(x, lags=lags, zero=False, ax=axes[1][1])\n",
    "    axes[1][0].set_xlabel('Lag')\n",
    "    axes[1][1].set_xlabel('Lag')\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    sns.despine()\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=.9)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    # if show_plot:\n",
    "    #     plt.show()\n",
    "    \n",
    "    \n",
    "def residual_analysis(residuals, y_pred, save_path=None):\n",
    "    \n",
    "    # Check for homoscedasticity\n",
    "    print(\"Homoscedasticity scatter plot:\")\n",
    "    plt.scatter(y_pred, residuals)\n",
    "    plt.xlabel(\"Predicted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.axhline(y=0, color=\"r\", linestyle=\"--\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    # Perform Ljung-Box test for autocorrelation in residuals\n",
    "    lb_test = acorr_ljungbox(residuals, lags=[10], return_df=True)  \n",
    "    print(\"Ljung-Box test for autocorrelation in residuals:\")\n",
    "    print(lb_test)\n",
    "    # Assess the p-value of the Ljung-Box test\n",
    "    p_value = lb_test['lb_pvalue'][10]\n",
    "    if p_value < 0.05:\n",
    "        print(\"The Ljung-Box test suggests that there is autocorrelation in the residuals (p-value < 0.05).\")\n",
    "    else:\n",
    "        print(\"The Ljung-Box test suggests that there is no significant autocorrelation in the residuals (p-value >= 0.05).\")\n",
    "    \n",
    "    # Perform Jarque-Bera test for normality in residuals\n",
    "    jb_test = jarque_bera(residuals)\n",
    "    print(\"\\nJarque-Bera test for normality in residuals:\")\n",
    "    if jb_test[1] < 0.05:\n",
    "        print(f\"Test statistic: {jb_test[0]}, p-value: {jb_test[1]}\")\n",
    "        print(\"The Jarque-Bera test suggests that the residuals are not normally distributed (p-value < 0.05).\")\n",
    "    else:\n",
    "        print(f\"Test statistic: {jb_test[0]}, p-value: {jb_test[1]}\")\n",
    "        print(\"The Jarque-Bera test suggests that the residuals are normally distributed (p-value >= 0.05).\")    \n",
    "        \n",
    "    # # Calculate the residuals\n",
    "    # residuals = y_train - y_pred\n",
    "\n",
    "    # # Plot the residuals\n",
    "    # fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    # ax.plot(residuals)\n",
    "    # ax.axhline(y=0, color='b', linestyle='-')\n",
    "    # ax.set_xlabel('Time')\n",
    "    # ax.set_ylabel('Residuals')\n",
    "    # ax.set_title('Residual Analysis')\n",
    "    # plt.show()\n",
    "    # print('Residual Analysis:')\n",
    "    # print('Mean of residuals:', round(residuals.mean(), 4))\n",
    "    # print('Standard deviation of residuals:', round(residuals.std(), 4))\n",
    "\n",
    "    # # Check for autocorrelation\n",
    "    # print(\"Autocorrelation plot:\")\n",
    "    # plot_acf(residuals)\n",
    "    # plt.show()\n",
    "\n",
    "    # # Check for normality\n",
    "    # print(\"Normality Q-Q plot:\")\n",
    "    # probplot(residuals, dist=\"norm\", plot=plt)\n",
    "    # plt.show()        \n",
    "            \n",
    "    # # Perform Breusch-Godfrey test for autocorrelation in residuals\n",
    "    # bg_test = acorr_breusch_godfrey(model, X_train, nlags=10)\n",
    "    # print(\"\\nBreusch-Godfrey test for autocorrelation in residuals:\")\n",
    "    # if bg_test[1] < 0.05:\n",
    "    #     print(f\"LM test statistic: {bg_test[0]}, p-value: {bg_test[1]}\")\n",
    "    #     print(\"The Breusch-Godfrey test suggests that there is autocorrelation in the residuals (p-value < 0.05).\")\n",
    "    # else:\n",
    "    #     print(f\"LM test statistic: {bg_test[0]}, p-value: {bg_test[1]}\")\n",
    "    #     print(\"The Breusch-Godfrey test suggests that there is no significant autocorrelation in the residuals (p-value >= 0.05).\")\n",
    "\n",
    "    # # Perform White test for heteroscedasticity in residuals\n",
    "    # white_test = het_white(residuals, X_train)\n",
    "    # print(\"\\nWhite test for heteroscedasticity in residuals:\")\n",
    "    # if white_test[1] < 0.05:\n",
    "    #     print(f\"Test statistic: {white_test[0]}, p-value: {white_test[1]}\")\n",
    "    #     print(\"The White test suggests that there is heteroscedasticity in the residuals (p-value < 0.05).\")\n",
    "    # else:\n",
    "    #     print(f\"Test statistic: {white_test[0]}, p-value: {white_test[1]}\")\n",
    "    #     print(\"The White test suggests that there is no significant heteroscedasticity in the residuals (p-value >= 0.05).\")\n",
    "\n",
    "    # # Perform Shapiro-Wilk test for normality in residuals\n",
    "    # sw_test = shapiro(residuals)\n",
    "    # print(\"\\nShapiro-Wilk test for normality in residuals:\")\n",
    "    # if sw_test[1] < 0.05:\n",
    "    #     print(f\"Test statistic: {sw_test[0]}, p-value: {sw_test[1]}\")\n",
    "    #     print(\"The Shapiro-Wilk test suggests that the residuals are not normally distributed (p-value < 0.05).\")\n",
    "    # else:\n",
    "    #     print(f\"Test statistic: {sw_test[0]}, p-value: {sw_test[1]}\")\n",
    "    #     print(\"The Shapiro-Wilk test suggests that the residuals are normally distributed (p-value >= 0.05).\")\n",
    "    \n",
    "    \n",
    "def plot_rmse_comparison(results, hypothesis, experiment):\n",
    "    fig = plt.figure()\n",
    "    model_names = list(results.keys())\n",
    "    train_rmse_values = [result['train_rmse'] for result in results.values()]\n",
    "    test_rmse_values = [result['test_rmse'] for result in results.values()]\n",
    "\n",
    "    ind = np.arange(len(model_names))  # the x locations for the groups\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(ind - width/2, train_rmse_values, width=width, label='Train Error')\n",
    "    plt.bar(ind + width/2, test_rmse_values, width=width, label='Test Error')\n",
    "    fig.set_size_inches(15, 8)\n",
    "    plt.legend()\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(model_names)\n",
    "    \n",
    "    # Save the plot\n",
    "    save_path = save_plots_for_experiment(experiment, hypothesis, \"Results\", \"\", \"Model_Comparison\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_param_grids = [\n",
    "    {\n",
    "        'model': DecisionTreeRegressor(random_state=42),\n",
    "        'param_grid': {\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'max_features': ['auto'],\n",
    "            'min_samples_leaf': [1, 3, 5, 10]\n",
    "        },\n",
    "        'model_name': 'CART',\n",
    "    },\n",
    "    # {\n",
    "    #     'model': MLPRegressor(random_state=42),\n",
    "    #     'param_grid': {\n",
    "    #         'hidden_layer_sizes': [(50,), (100,), (50, 50),(100, 100), (50, 50, 50), (100, 100, 100), (50, 50, 50, 50), (100, 100, 100, 100)],\n",
    "    #         'activation': ['tanh', 'relu', 'logistic'],\n",
    "    #         'solver': ['sgd'],\n",
    "    #         'alpha': [0.00005, 0.0005, 0.005],\n",
    "    #         'early_stopping': [True],\n",
    "    #         'max_iter': [600],\n",
    "    #         'shuffle': [False],\n",
    "    #     },\n",
    "    #     'model_name': 'MLP',\n",
    "    # },\n",
    "    {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'param_grid': {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto'],\n",
    "            'leaf_size': [10, 30, 50],\n",
    "        },\n",
    "        'model_name': 'KNN',\n",
    "    },\n",
    "    # {\n",
    "    #     'model': GaussianProcessRegressor(random_state=42),\n",
    "    #     'param_grid': {\n",
    "    #         'kernel': [RBF(), DotProduct()+ WhiteKernel()],\n",
    "    #         'alpha': [1e-10, 1e-5, 1e-2, 1],\n",
    "    #         'n_restarts_optimizer': [0, 1, 3],\n",
    "    #     },\n",
    "    #     'model_name': 'GPR',\n",
    "    # },\n",
    "    # {\n",
    "    #     'model': lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42),\n",
    "    #     'param_grid': {\n",
    "    #         'lgbmregressor__n_estimators': [100],\n",
    "    #         'lgbmregressor__learning_rate': [0.01],\n",
    "    #         'lgbmregressor__max_depth': [5, 10, 20],\n",
    "    #         'lgbmregressor__num_leaves': [31, 50],\n",
    "    #     },\n",
    "    #     'model_name': 'GBR',\n",
    "    # },\n",
    "    {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'param_grid': {\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'max_features': ['auto'],\n",
    "        },\n",
    "        'model_name': 'RF',\n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=30, test_size=14)\n",
    "lags = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split train/test\n",
    "train_data, test_data = train_test_split(rt_d, test_size=0.2, shuffle=False)\n",
    "#set up data for supervised learning for H1: ts1 pred ts1\n",
    "train_transformed = prepare_time_series_data(train_data, 'ts1', lags)\n",
    "test_transformed = prepare_time_series_data(test_data, 'ts1', lags)\n",
    "train_transformed = train_transformed.drop(['ts2'], axis=1)\n",
    "test_transformed = test_transformed.drop(['ts2'], axis=1)\n",
    "# 2. Scale based on train set\n",
    "train_data_scaled, test_data_scaled, scaler_X, scaler_Y = scale_data(train_transformed, test_transformed, 'ts1')\n",
    "# 3. Create supervised learning data by adding datetime features and lagged features\n",
    "train_data_df = pd.DataFrame(train_data_scaled, index=train_transformed.index, columns=train_transformed.columns)\n",
    "test_data_df = pd.DataFrame(test_data_scaled, index=test_transformed.index, columns=test_transformed.columns)\n",
    "X_train, Y_train = extract_column(train_data_df, 'ts1')\n",
    "X_test, Y_test = extract_column(test_data_df, 'ts1')\n",
    "\n",
    "hypothesis = 'H1'\n",
    "experiment = 'Experiment1'\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_info in models_and_param_grids:\n",
    "    model = model_info['model']\n",
    "    param_grid = model_info['param_grid']\n",
    "    model_name = model_info['model_name']\n",
    "    print(f\"\\nTraining {model_name} model...\")\n",
    "    \n",
    "    best_model_trained, train_rmse, test_rmse = model_pipeline(\n",
    "        model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, hypothesis, model_name, experiment=experiment\n",
    "    )\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': best_model_trained,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "    }\n",
    "    \n",
    "print(\"\\nResults:\")\n",
    "for model_name, model_results in results.items():\n",
    "    print(\n",
    "        f\"{model_name}: Train RMSE: {model_results['train_rmse']:.4f}, Test RMSE: {model_results['test_rmse']:.4f}\"\n",
    "    )\n",
    "    \n",
    "plot_rmse_comparison(results, hypothesis, experiment=experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split train/test\n",
    "train_data, test_data = train_test_split(rt_d, test_size=0.2, shuffle=False)\n",
    "#set up data for supervised learning for H1: ts1 pred ts1\n",
    "train_transformed = prepare_time_series_data(train_data, 'ts2', lags)\n",
    "test_transformed = prepare_time_series_data(test_data, 'ts2', lags)\n",
    "train_transformed = train_transformed.drop(['ts1'], axis=1)\n",
    "test_transformed = test_transformed.drop(['ts1'], axis=1)\n",
    "# 2. Scale based on train set\n",
    "train_data_scaled, test_data_scaled, scaler_X, scaler_Y = scale_data(train_transformed, test_transformed, 'ts2')\n",
    "# 3. Create supervised learning data by adding datetime features and lagged features\n",
    "train_data_df = pd.DataFrame(train_data_scaled, index=train_transformed.index, columns=train_transformed.columns)\n",
    "test_data_df = pd.DataFrame(test_data_scaled, index=test_transformed.index, columns=test_transformed.columns)\n",
    "X_train, Y_train = extract_column(train_data_df, 'ts2')\n",
    "X_test, Y_test = extract_column(test_data_df, 'ts2')\n",
    "\n",
    "\n",
    "hypothesis = 'H2'\n",
    "experiment = 'Experiment1'\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_info in models_and_param_grids:\n",
    "    model = model_info['model']\n",
    "    param_grid = model_info['param_grid']\n",
    "    model_name = model_info['model_name']\n",
    "    print(f\"\\nTraining {model_name} model...\")\n",
    "    \n",
    "    best_model_trained, train_rmse, test_rmse = model_pipeline(\n",
    "        model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, hypothesis, model_name, experiment=experiment\n",
    "    )\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': best_model_trained,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "    }\n",
    "    \n",
    "print(\"\\nResults:\")\n",
    "for model_name, model_results in results.items():\n",
    "    print(\n",
    "        f\"{model_name}: Train RMSE: {model_results['train_rmse']:.4f}, Test RMSE: {model_results['test_rmse']:.4f}\"\n",
    "    )\n",
    "    \n",
    "plot_rmse_comparison(results, hypothesis, experiment=experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split train/test\n",
    "train_data, test_data = train_test_split(rt_d, test_size=0.2, shuffle=False)\n",
    "#set up data for supervised learning for H1: ts1 pred ts1\n",
    "train_transformed = prepare_time_series_data(train_data, 'ts1', lags)\n",
    "test_transformed = prepare_time_series_data(test_data, 'ts1', lags)\n",
    "train_transformed = train_transformed.drop(['ts1'], axis=1)\n",
    "test_transformed = test_transformed.drop(['ts1'], axis=1)\n",
    "# 2. Scale based on train set\n",
    "train_data_scaled, test_data_scaled, scaler_X, scaler_Y = scale_data(train_transformed, test_transformed, 'ts2')\n",
    "# 3. Create supervised learning data by adding datetime features and lagged features\n",
    "train_data_df = pd.DataFrame(train_data_scaled, index=train_transformed.index, columns=train_transformed.columns)\n",
    "test_data_df = pd.DataFrame(test_data_scaled, index=test_transformed.index, columns=test_transformed.columns)\n",
    "X_train, Y_train = extract_column(train_data_df, 'ts2')\n",
    "X_test, Y_test = extract_column(test_data_df, 'ts2')\n",
    "\n",
    "\n",
    "hypothesis = 'H3'\n",
    "experiment = 'Experiment1'\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_info in models_and_param_grids:\n",
    "    model = model_info['model']\n",
    "    param_grid = model_info['param_grid']\n",
    "    model_name = model_info['model_name']\n",
    "    print(f\"\\nTraining {model_name} model...\")\n",
    "    \n",
    "    best_model_trained, train_rmse, test_rmse = model_pipeline(\n",
    "        model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, hypothesis, model_name, experiment=experiment\n",
    "    )\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': best_model_trained,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "    }\n",
    "    \n",
    "print(\"\\nResults:\")\n",
    "for model_name, model_results in results.items():\n",
    "    print(\n",
    "        f\"{model_name}: Train RMSE: {model_results['train_rmse']:.4f}, Test RMSE: {model_results['test_rmse']:.4f}\"\n",
    "    )\n",
    "    \n",
    "plot_rmse_comparison(results, hypothesis, experiment=experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split train/test\n",
    "train_data, test_data = train_test_split(rt_d, test_size=0.2, shuffle=False)\n",
    "#set up data for supervised learning for H1: ts1 pred ts1\n",
    "train_transformed = prepare_time_series_data(train_data, 'ts2', lags)\n",
    "test_transformed = prepare_time_series_data(test_data, 'ts2', lags)\n",
    "train_transformed = train_transformed.drop(['ts2'], axis=1)\n",
    "test_transformed = test_transformed.drop(['ts2'], axis=1)\n",
    "# 2. Scale based on train set\n",
    "train_data_scaled, test_data_scaled, scaler_X, scaler_Y = scale_data(train_transformed, test_transformed, 'ts1')\n",
    "# 3. Create supervised learning data by adding datetime features and lagged features\n",
    "train_data_df = pd.DataFrame(train_data_scaled, index=train_transformed.index, columns=train_transformed.columns)\n",
    "test_data_df = pd.DataFrame(test_data_scaled, index=test_transformed.index, columns=test_transformed.columns)\n",
    "X_train, Y_train = extract_column(train_data_df, 'ts1')\n",
    "X_test, Y_test = extract_column(test_data_df, 'ts1')\n",
    "\n",
    "\n",
    "hypothesis = 'H4'\n",
    "experiment = 'Experiment1'\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_info in models_and_param_grids:\n",
    "    model = model_info['model']\n",
    "    param_grid = model_info['param_grid']\n",
    "    model_name = model_info['model_name']\n",
    "    print(f\"\\nTraining {model_name} model...\")\n",
    "    \n",
    "    best_model_trained, train_rmse, test_rmse = model_pipeline(\n",
    "        model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, hypothesis, model_name, experiment=experiment\n",
    "    )\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': best_model_trained,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "    }\n",
    "    \n",
    "print(\"\\nResults:\")\n",
    "for model_name, model_results in results.items():\n",
    "    print(\n",
    "        f\"{model_name}: Train RMSE: {model_results['train_rmse']:.4f}, Test RMSE: {model_results['test_rmse']:.4f}\"\n",
    "    )\n",
    "    \n",
    "plot_rmse_comparison(results, hypothesis, experiment=experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rt_transformed = prepare_time_series_data(rt_d, 'ts1', 2)\n",
    "# rt_transformed = rt_transformed.drop(['ts2'], axis=1)\n",
    "# X, Y = extract_column(rt_transformed, 'ts1')\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
    "# train_data = pd.concat([X_train, Y_train], axis=1)\n",
    "# test_data = pd.concat([X_test, Y_test], axis=1)\n",
    "# train_data_scaled, test_data_scaled = scale_data(train_data, test_data)\n",
    "# train_data_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=30, test_size=14)\n",
    "#find best model\n",
    "param_grid = { 'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'max_features': ['auto']}\n",
    "model = RandomForestRegressor()\n",
    "best_model_trained = model_pipeline(model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y,'H1','RF')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "tscv = TimeSeriesSplit(n_splits=40, test_size=14)\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 30, 50],\n",
    "}\n",
    "model_knn = KNeighborsRegressor()\n",
    "best_model_trained_knn = model_pipeline(model_knn, param_grid_knn, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, 'H1', 'KNN')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_param_grid = {\n",
    "    'max_depth': [None, 3, 5, 7, 9,11],\n",
    "    'min_samples_split': [2, 5, 10,20],\n",
    "    'min_samples_leaf': [1, 3, 5, 10]\n",
    "}\n",
    "\n",
    "model_cart = DecisionTreeRegressor()\n",
    "best_model_trained_cart = model_pipeline(model_cart, cart_param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, 'H1', 'CART')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "\n",
    "# param_grid_svr = {\n",
    "#     'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "#     'C': [0.1, 1, 10],\n",
    "#     'epsilon': [0.01, 0.1, 1,10],\n",
    "#     'gamma': [0.001, 0.01, 0.1, 1, 2, 5],\n",
    "# }\n",
    "# model_svr = SVR()\n",
    "# best_model_trained_svr = model_pipeline(model_svr, param_grid_svr, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, 'H1', 'SVR')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_GBR = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42)\n",
    "param_grid_GBM = {\n",
    "    'lgbmregressor__n_estimators': [100],\n",
    "    'lgbmregressor__learning_rate': [0.01],\n",
    "    'lgbmregressor__max_depth': [5, 10,20],\n",
    "    'lgbmregressor__num_leaves': [31, 50],\n",
    "\n",
    "}\n",
    "\n",
    "best_model_trained_BGM = model_pipeline(model_GBR, param_grid_GBM, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, 'H1', 'GBR')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid_gpr = {\n",
    "    'kernel': [DotProduct()+ WhiteKernel()],\n",
    "    'alpha': [1e-10, 1e-5, 1e-2, 1],\n",
    "    'n_restarts_optimizer': [0, 1, 3],\n",
    "}\n",
    "model_gpr = GaussianProcessRegressor()\n",
    "best_model_trained_gpr = model_pipeline(model_gpr, param_grid_gpr, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, 'H1', 'GPR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50),(100, 100), (50, 50, 50), (100, 100, 100), (50, 50, 50, 50), (100, 100, 100, 100)],\n",
    "    'activation': ['tanh', 'relu', 'logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'early_stopping': [True],\n",
    "    'max_iter': [600],\n",
    "    'shuffle': [False],\n",
    "}\n",
    "model_mlp = MLPRegressor()\n",
    "best_model_trained_mlp = model_pipeline(model_mlp, param_grid_mlp, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, 'H1_test', 'MLP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split train/test\n",
    "train_data, test_data = train_test_split(rt_d, test_size=0.2, shuffle=False)\n",
    "# 2. Scale based on train set\n",
    "train_data_scaled, test_data_scaled, scaler_X, scaler_Y = scale_data(train_data, test_data)\n",
    "# 3. Create supervised learning data by adding datetime features and lagged features\n",
    "train_data_df = pd.DataFrame(train_data_scaled, index=train_data.index, columns=train_data.columns)\n",
    "test_data_df = pd.DataFrame(test_data_scaled, index=test_data.index, columns=test_data.columns)\n",
    "#set up data for supervised learning for H1: ts1 pred ts1\n",
    "train_transformed = prepare_time_series_data(train_data_df, 'ts1', 2)\n",
    "test_transformed = prepare_time_series_data(test_data_df, 'ts1', 2)\n",
    "train_transformed = train_transformed.drop(['ts2'], axis=1)\n",
    "test_transformed = test_transformed.drop(['ts2'], axis=1)\n",
    "X_train, Y_train = extract_column(train_transformed, 'ts1')\n",
    "X_test, Y_test = extract_column(test_transformed, 'ts1')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=30, test_size=14)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_info in models_and_param_grids:\n",
    "    model = model_info['model']\n",
    "    param_grid = model_info['param_grid']\n",
    "    model_name = model_info['model_name']\n",
    "    print(f\"\\nTraining {model_name} model...\")\n",
    "    \n",
    "    best_model_trained, train_rmse, test_rmse = model_pipeline(\n",
    "        model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y, 'H1', model_name\n",
    "    )\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'model': best_model_trained,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "    }\n",
    "    \n",
    "print(\"\\nResults:\")\n",
    "for model_name, model_results in results.items():\n",
    "    print(\n",
    "        f\"{model_name}: Train RMSE: {model_results['train_rmse']:.4f}, Test RMSE: {model_results['test_rmse']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "model_names = list(results.keys())\n",
    "train_rmse_values = [result['train_rmse'] for result in results.values()]\n",
    "test_rmse_values = [result['test_rmse'] for result in results.values()]\n",
    "\n",
    "ind = np.arange(len(model_names))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.bar(ind - width/2, train_rmse_values, width=width, label='Train Error')\n",
    "plt.bar(ind + width/2, test_rmse_values, width=width, label='Test Error')\n",
    "fig.set_size_inches(15, 8)\n",
    "plt.legend()\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(model_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pmdarima.model_selection import RollingForecastCV\n",
    "# rfcv = RollingForecastCV(\n",
    "#     h=30,  # forecast horizon\n",
    "#     step=30,  # step size\n",
    "# )\n",
    "# i=0\n",
    "# for train_index, test_index in rfcv.split(X_train):\n",
    "#     i+=1\n",
    "#     print(f\"fold {i}\\n\")\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(model, param_grid, X_train, Y_train, cv):\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        verbose=2,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    mean_score = -np.mean(cv_results['mean_test_score'])\n",
    "    std_score = np.std(cv_results['mean_test_score'])\n",
    "    \n",
    "    return best_model, mean_score, std_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "param_grid = {'n_estimators': [10, 50, 100, 200]}\n",
    "tscv = TimeSeriesSplit(n_splits=30, test_size=14)\n",
    "best_model, mean_error, std_error = perform_grid_search(model, param_grid, X_train, Y_train, tscv)\n",
    "print(\"Best model:\", best_model)\n",
    "print(\"Mean error:\", mean_error)\n",
    "print(\"Standard error:\", std_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=30, test_size=14)\n",
    "# for i, (train_index, test_index) in enumerate(tscv.split(X_train)):\n",
    "#     print(f\"fold {i}\\n\")\n",
    "#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "# # #random forest\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, Y_train)\n",
    "# preds = model.predict(X_train)\n",
    "# mse = evaluate(Y_train, preds)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid = { 'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'max_features': ['auto']}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "cv_results = grid_search.cv_results_\n",
    "mean_score = -np.mean(cv_results['mean_test_score'])\n",
    "std_score = np.std(cv_results['mean_test_score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactor code in a logical way such that it follows a sequence of functions applied to my already defined training and testing data:\n",
    "-   Grid search cv\n",
    "-   best model mean and std\n",
    "-   train on all training\n",
    "-   predict on train\n",
    "-   plot resid + squared resid of unscaled predictions\n",
    "-   predict on test\n",
    "-   plot resid + squared residual\" of unscaled predictions\n",
    "\n",
    "write the return as would be required in the sequence of the pipeline as if there were function calls in the main body of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_unscaled_predictions(preds, actuals, scaler_Y, title=\"Unscaled Predictions vs Actual Data\", residual_analysis_func=None):\n",
    "    # Reshape predictions and actual values for unscaling\n",
    "    preds_reshaped = preds.reshape(-1, 1)\n",
    "    actuals_reshaped = actuals.values.reshape(-1, 1)\n",
    "\n",
    "    # Unscale the predictions and actual values\n",
    "    unscaled_preds, unscaled_actuals = unscale_data(preds_reshaped, actuals_reshaped, scaler_Y)\n",
    "\n",
    "    # Convert the unscaled predictions and actuals back to pandas Series\n",
    "    unscaled_preds_series = pd.Series(unscaled_preds.squeeze(), index=actuals.index)\n",
    "    unscaled_actuals_series = pd.Series(unscaled_actuals.squeeze(), index=actuals.index)\n",
    "\n",
    "    # Plot unscaled predictions against the actual data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(unscaled_actuals_series.index, unscaled_actuals_series, label=\"Actual\")\n",
    "    ax.plot(unscaled_preds_series.index, unscaled_preds_series, label=\"Predicted\", linestyle=\"--\")\n",
    "    ax.legend()\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    # Perform residual analysis if a function is provided\n",
    "    if residual_analysis_func is not None:\n",
    "        residual_analysis_func(unscaled_preds_series, unscaled_actuals_series)\n",
    "\n",
    "preds = best_model.predict(X_train)\n",
    "print(mean_score)\n",
    "plot_unscaled_predictions(preds, Y_train, scaler_Y, title=\"Unscaled Predictions vs Actual Training Data\", residual_analysis_func=residual_analysis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=30, test_size=14)\n",
    "#find best model\n",
    "param_grid = { 'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'max_features': ['auto']}\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "best_model_trained = model_pipeline(model, param_grid, X_train, Y_train, X_test, Y_test, tscv, scaler_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=30, test_size=14)\n",
    "#find best model\n",
    "param_grid = { 'n_estimators': [10, 50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'max_features': ['auto']}\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "best_model, mean_error, std_error = perform_grid_search(model, param_grid, X_train, Y_train, tscv)\n",
    "print(\"Best model:\", best_model)\n",
    "print(\"Mean error:\", mean_error)\n",
    "print(\"Standard error:\", std_error)\n",
    "# Train best model on all training data\n",
    "best_model_trained = train_best_model(best_model, X_train, Y_train)\n",
    "# Predict on train\n",
    "train_preds = predict(best_model_trained, X_train)\n",
    "# Plot predictions vs actuals\n",
    "unscaled_preds_series, unscaled_actuals_series = reshape_and_unscale_predictions(train_preds, Y_train, scaler_Y)\n",
    "# Plot resid + squared resid of unscaled predictions (train)\n",
    "# Calculate the residuals\n",
    "residuals = unscaled_actuals_series - unscaled_preds_series\n",
    "residual_analysis(residuals,unscaled_preds_series)\n",
    "residual_analysis(residuals.pow(2), unscaled_preds_series.pow(2))\n",
    "\n",
    "\n",
    "# Predict on test\n",
    "test_preds = predict(best_model_trained, X_test)\n",
    "# Plot resid + squared residual of unscaled predictions (test)\n",
    "unscaled_preds_series, unscaled_actuals_series = reshape_and_unscale_predictions(test_preds, Y_test, scaler_Y)\n",
    "# Plot resid + squared resid of unscaled predictions (train)\n",
    "# Calculate the residuals\n",
    "residuals = unscaled_actuals_series - unscaled_preds_series\n",
    "residual_analysis(residuals,unscaled_preds_series)\n",
    "residual_analysis(residuals.pow(2), unscaled_preds_series.pow(2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "best_model, mean_error, std_error = perform_grid_search(model, param_grid, X_train, Y_train, tscv)\n",
    "\n",
    "print(\"Best model:\", best_model)\n",
    "print(\"Mean error:\", mean_error)\n",
    "print(\"Standard error:\", std_error)\n",
    "\n",
    "# Train best model on all training data\n",
    "best_model_trained = train_best_model(best_model, X_train, Y_train)\n",
    "\n",
    "# Predict on train set\n",
    "train_preds = predict(best_model_trained, X_train)\n",
    "\n",
    "# Unscale and plot predictions vs actuals for train set\n",
    "unscaled_preds_series, unscaled_actuals_series = reshape_and_unscale_predictions(train_preds, Y_train, scaler_Y)\n",
    "plot_series(unscaled_actuals_series, unscaled_preds_series, title=\"Unscaled Predictions vs Actual Training Data\")\n",
    "\n",
    "# Perform residual analysis for train set\n",
    "residuals_train = unscaled_actuals_series - unscaled_preds_series\n",
    "residual_analysis(residuals_train, unscaled_preds_series)\n",
    "residual_analysis(residuals_train.pow(2), unscaled_preds_series.pow(2))\n",
    "\n",
    "# Predict on test set\n",
    "test_preds = predict(best_model_trained, X_test)\n",
    "\n",
    "# Unscale and plot predictions vs actuals for test set\n",
    "unscaled_preds_series, unscaled_actuals_series = reshape_and_unscale_predictions(test_preds, Y_test, scaler_Y)\n",
    "plot_series(unscaled_actuals_series, unscaled_preds_series, title=\"Unscaled Predictions vs Actual Test Data\")\n",
    "\n",
    "# Perform residual analysis for test set\n",
    "residuals_test = unscaled_actuals_series - unscaled_preds_series\n",
    "residual_analysis(residuals_test, unscaled_preds_series)\n",
    "residual_analysis(residuals_test.pow(2), unscaled_preds_series.pow(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = best_model.predict(X_train)\n",
    "print(mean_score)\n",
    "plot_unscaled_predictions(preds, Y_train, scaler_Y, title=\"Unscaled Predictions vs Actual Training Data\", residual_analysis_func=residual_analysis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'epsilon': [0.01, 0.1, 1]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=-1, random_state=42)\n",
    "param_grid = {\n",
    "    'lgbmregressor__n_estimators': [100, 200, 500],\n",
    "    'lgbmregressor__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'lgbmregressor__max_depth': [-1, 5, 10],\n",
    "    'lgbmregressor__num_leaves': [31, 50, 100],\n",
    "    'lgbmregressor__min_child_samples': [10, 20, 30],\n",
    "    'lgbmregressor__subsample': [0.8, 0.9, 1.0],\n",
    "    'lgbmregressor__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'lgbmregressor__reg_alpha': [0.0, 0.1, 0.5],\n",
    "    'lgbmregressor__reg_lambda': [0.0, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (200,), (50, 50)],\n",
    "    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_param_grid = {\n",
    "    'alpha': [1e-10, 1e-9, 1e-8, 1e-7],\n",
    "    'optimizer': ['fmin_l_bfgs_b', 'fmin_cg', 'fmin_ncg', None],\n",
    "    'n_restarts_optimizer': [0, 1, 2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "rfcv = RollingForecastCV(\n",
    "    h=170,  # forecast horizon\n",
    "    step=30,  # step size\n",
    ")\n",
    "\n",
    "def grid_search(data, cfg_list):\n",
    "    scores = {}\n",
    "    models = {}\n",
    "\n",
    "\n",
    "    # score, model = \n",
    "    scores[cfg] = score\n",
    "    models[cfg] = model\n",
    "\n",
    "    # Calculate mean and standard deviation of scores\n",
    "    mean_score = np.mean(list(scores.values()))\n",
    "    std_score = np.std(list(scores.values()))\n",
    "\n",
    "    # Find the best configuration and model using argmin\n",
    "    best_cfg = min(scores, key=scores.get)\n",
    "    best_model = models[best_cfg]\n",
    "    \n",
    "    return best_model, scores, models, mean_score, std_score\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # msle = mean_squared_log_error(y_true, y_pred)\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "    }\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "def get_best_model_mean_std(grid_search_result):\n",
    "    # Extract mean and standard deviation of the best model\n",
    "    \n",
    "    return best_model_mean, best_model_std\n",
    "\n",
    "def train_best_model(best_estimator, X_train, y_train):\n",
    "        # Train the best estimator on all training data\n",
    "    fitted_model = best_estimator.fit(X_train, y_train)\n",
    "    \n",
    "    return fitted_model\n",
    "\n",
    "def predict_on_train(fitted_model, X_train):\n",
    "    # Predict on the training data\n",
    "    # ...\n",
    "    return train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(data, cfg_list):\n",
    "    scores_and_models = {}\n",
    "    for cfg in cfg_list:\n",
    "        score, model = walk_forward_validation(data, cfg)\n",
    "        scores_and_models[cfg] = {'score': score, 'model': model}\n",
    "    \n",
    "    # Calculate mean and standard deviation of scores\n",
    "    scores = [v['score'] for v in scores_and_models.values()]\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    \n",
    "    return scores_and_models, mean_score, std_score\n",
    "\n",
    "\n",
    "def train_model(train, config):\n",
    "    order = config\n",
    "    model = ARIMA(train, order=order)\n",
    "    model_fit = model.fit()\n",
    "    return model_fit\n",
    "\n",
    "def model_predict(model, start, end):\n",
    "    return model.predict(start=start, end=end)\n",
    "\n",
    "# def evaluate(test_data, prediction):\n",
    "#     return measure_rmse(test_data, prediction)\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # msle = mean_squared_log_error(y_true, y_pred)\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "    }\n",
    "\n",
    "def walk_forward_validation(data, cfg):\n",
    "    rfcv = RollingForecastCV(h=170, step=30)\n",
    "    evaluation_results = []\n",
    "\n",
    "    for train_index, test_index in rfcv.split(data):\n",
    "        train_data = data[train_index]\n",
    "        test_data = data[test_index]\n",
    "        model_fit = train_model(train_data, cfg)\n",
    "        prediction = model_predict(model_fit, start=len(train_data), end=len(train_data)+len(test_data)-1)\n",
    "        evaluation_result = evaluate(test_data, prediction)\n",
    "        evaluation_results.append(evaluation_result)\n",
    "\n",
    "    return np.mean(evaluation_results), model_fit\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the data to supervised regression format\n",
    "All the predictor variables are changed to lagged variable, as the t-1 value of the lagged variable will be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Create a 60-days window of historical prices (i-60) as our feature data (x_train) and the following 60-days window as label data (y_train).\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "    \n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "#Extract the closing prices from our normalized dataset (the last 20% of the dataset).\n",
    "test_data = scaled_data[training_data_len-60: , : ]\n",
    "x_test = []\n",
    "y_test = values[training_data_len:]\n",
    "\n",
    "for i in range(60, len(test_data)):\n",
    "  x_test.append(test_data[i-60:i, 0])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "\n",
    "def create_features_and_targets(data, feature_length):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(data) - feature_length - 1):\n",
    "        X.append(data[i:(i+feature_length), 0])\n",
    "        Y.append(data[i + feature_length, 0])\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "# calling the function\n",
    "X_train, y_train = create_features_and_targets(dataV1, feature_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test options for regression\n",
    "num_folds = 10\n",
    "scoring = 'neg_mean_squared_error'\n",
    "#scoring ='neg_mean_absolute_error'\n",
    "#scoring = 'r2'\n",
    "# spot check the algorithms\n",
    "models = []\n",
    "models.append(('LR', LinearRegression()))\n",
    "models.append(('LASSO', Lasso()))\n",
    "models.append(('EN', ElasticNet()))\n",
    "models.append(('KNN', KNeighborsRegressor()))\n",
    "models.append(('CART', DecisionTreeRegressor()))\n",
    "models.append(('SVR', SVR()))\n",
    "#Neural Network\n",
    "models.append(('MLP', MLPRegressor()))\n",
    "models.append(('GBR', GradientBoostingRegressor()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unscale_data(pred, actual):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(actual)\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    actual = scaler.inverse_transform(actual)\n",
    "    return pred, actual\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "data = stock_data.filter(['Close'])\n",
    "train = data[:training_data_len]\n",
    "validation = data[training_data_len:]\n",
    "validation['Predictions'] = predictions\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price USD ($)')\n",
    "plt.plot(train)\n",
    "plt.plot(validation[['Close', 'Predictions']])\n",
    "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    # msle = mean_squared_log_error(y_true, y_pred)\n",
    "    return {\n",
    "        'rmse': rmse,\n",
    "    }\n",
    "    \n",
    "# def diagnostics(model, data1, data2):\n",
    "#     # perform diagnostics and return the print statements\n",
    "#     print('Diagnostics:')\n",
    "#     print(model.summary())\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#plot results\n",
    "def plot_results(cols, results, data_name):\n",
    "    for row in results[cols].iterrows():\n",
    "        yhat, resid, actual, name = row[1]\n",
    "        plt.title(f'{data_name} - {name}')\n",
    "        plt.plot(actual, 'k--', alpha=0.5)\n",
    "        plt.plot(yhat, 'k')\n",
    "        plt.legend(['actual', 'forecast'])\n",
    "        plot_acf(resid, zero=False, \n",
    "                 title=f'{data_name} - Autocorrelation')\n",
    "        plt.show()\n",
    "\n",
    "# cols = ['yhat', 'resid', 'actual', 'Model Name']\n",
    "# plot_results(cols, air_results, 'Air Passengers')\n",
    "    \n",
    "\n",
    "\n",
    "# def plot_correlogram(x, lags=None, title=None):\n",
    "#     lags = min(10, int(len(x)/5)) if lags is None else lags\n",
    "#     with sns.axes_style('whitegrid'):\n",
    "#         fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 8))\n",
    "#         x.plot(ax=axes[0][0], title='Residuals')\n",
    "#         x.rolling(21).mean().plot(ax=axes[0][0], c='k', lw=1)\n",
    "#         q_p = np.max(q_stat(acf(x, nlags=lags), len(x))[1])\n",
    "#         stats = f'Q-Stat: {np.max(q_p):>8.2f}\\nADF: {adfuller(x)[1]:>11.2f}'\n",
    "#         axes[0][0].text(x=.02, y=.85, s=stats, transform=axes[0][0].transAxes)\n",
    "#         probplot(x, plot=axes[0][1])\n",
    "#         mean, var, skew, kurtosis = moment(x, moment=[1, 2, 3, 4])\n",
    "#         s = f'Mean: {mean:>12.2f}\\nSD: {np.sqrt(var):>16.2f}\\nSkew: {skew:12.2f}\\nKurtosis:{kurtosis:9.2f}'\n",
    "#         axes[0][1].text(x=.02, y=.75, s=s, transform=axes[0][1].transAxes)\n",
    "#         plot_acf(x=x, lags=lags, zero=False, ax=axes[1][0])\n",
    "#         plot_pacf(x, lags=lags, zero=False, ax=axes[1][1])\n",
    "#         axes[1][0].set_xlabel('Lag')\n",
    "#         axes[1][1].set_xlabel('Lag')\n",
    "#         fig.suptitle(title, fontsize=14)\n",
    "#         sns.despine()\n",
    "#         fig.tight_layout()\n",
    "#         fig.subplots_adjust(top=.9)\n",
    "        \n",
    "        \n",
    "# #plot stats\n",
    "# error = (y - y_pred).rename('Prediction Errors')\n",
    "# scores = dict(\n",
    "#     rmse=np.sqrt(mean_squared_error(y_true=y, y_pred=y_pred)),\n",
    "#     rmsle=np.sqrt(mean_squared_log_error(y_true=y, y_pred=y_pred)),\n",
    "#     mean_ae=mean_absolute_error(y_true=y, y_pred=y_pred),\n",
    "#     median_ae=median_absolute_error(y_true=y, y_pred=y_pred),\n",
    "#     r2score=explained_variance_score(y_true=y, y_pred=y_pred)\n",
    "# )\n",
    "# fig, axes = plt.subplots(ncols=3, figsize=(15, 4))\n",
    "# sns.scatterplot(x=y, y=y_pred, ax=axes[0])\n",
    "# axes[0].set_xlabel('Log Price')\n",
    "# axes[0].set_ylabel('Predictions')\n",
    "# axes[0].set_ylim(11, 16)\n",
    "# axes[0].set_title('Predicted vs. Actuals')\n",
    "# sns.distplot(error, ax=axes[1])\n",
    "# axes[1].set_title('Residuals')\n",
    "# pd.Series(scores).plot.barh(ax=axes[2], title='Error Metrics')\n",
    "# fig.suptitle('In-Sample Regression Errors', fontsize=16)\n",
    "# sns.despine()\n",
    "# fig.tight_layout()\n",
    "# fig.subplots_adjust(top=.88)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(rt_d, x_var, y_var, test_size=0.2, max_lag=2):\n",
    "    # 1. Split train/test\n",
    "    train_data, test_data = train_test_split(rt_d, test_size=test_size, shuffle=False)\n",
    "    \n",
    "    # 2. Scale based on train set\n",
    "    train_data_scaled, test_data_scaled = scale_data(train_data, test_data)\n",
    "    \n",
    "    # 3. Create supervised learning data by adding datetime features and lagged features\n",
    "    train_data_df = pd.DataFrame(train_data_scaled, index=train_data.index, columns=train_data.columns)\n",
    "    test_data_df = pd.DataFrame(test_data_scaled, index=test_data.index, columns=test_data.columns)\n",
    "    \n",
    "    # Set up data for supervised learning\n",
    "    train_transformed = prepare_time_series_data(train_data_df, y_var, max_lag)\n",
    "    test_transformed = prepare_time_series_data(test_data_df, y_var, max_lag)\n",
    "    \n",
    "    # Extract X and Y from the transformed train and test sets\n",
    "    X_train, Y_train = extract_column(train_transformed, y_var)\n",
    "    X_test, Y_test = extract_column(test_transformed, y_var)\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "X_train_ts1_ts1, Y_train_ts1_ts1, X_test_ts1_ts1, Y_test_ts1_ts1 = prepare_data_for_modeling(rt_d, 'ts1', 'ts1')\n",
    "X_train_ts2_ts2, Y_train_ts2_ts2, X_test_ts2_ts2, Y_test_ts2_ts2 = prepare_data_for_modeling(rt_d, 'ts2', 'ts2')\n",
    "X_train_ts1_ts2, Y_train_ts1_ts2, X_test_ts1_ts2, Y_test_ts1_ts2 = prepare_data_for_modeling(rt_d, 'ts1', 'ts2')\n",
    "X_train_ts2_ts1, Y_train_ts2_ts1, X_test_ts2_ts1, Y_test_ts2_ts1 = prepare_data_for_modeling(rt_d, 'ts2', 'ts1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(train_x, train_y, test_x, test_y):\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(train_x, train_y)\n",
    "    preds = model.predict(test_x)\n",
    "    mse, mae, mape = evaluate_predictions(test_y, preds)\n",
    "\n",
    "    print(\"Random Forest\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}\")\n",
    "    \n",
    "    return mse, mae, mape\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n",
    "scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "pd.Series(np.sqrt(-scores)).describe()\n",
    "#####\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "n_estimators : integer, optional (default=10)\n",
    "    The number of trees in the forest.\n",
    "'''\n",
    "param_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\n",
    "model = RandomForestRegressor()\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "param_grid = {'n_estimators': [200, 400],\n",
    "              'max_depth': [10, 15, 20],\n",
    "              'min_samples_leaf': [50, 100],\n",
    "              }\n",
    "gridsearch_reg = GridSearchCV(estimator=rf_reg,\n",
    "                              param_grid=param_grid,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              n_jobs=-1,\n",
    "                              cv=cv,\n",
    "                              refit=True,\n",
    "                              return_train_score=True,\n",
    "                              verbose=1)\n",
    "\n",
    "param_grid = [\n",
    "    # try 12 (34) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (23) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "grid_search.best_params_\n",
    "grid_search.best_estimator_\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_reg = SVR(kernel=\"linear\")\n",
    "svm_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = svm_reg.predict(housing_prepared)\n",
    "svm_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svm_rmse = np.sqrt(svm_mse)\n",
    "svm_rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lightgbm(train_x, train_y, test_x, test_y):\n",
    "    # Fit LightGBM model\n",
    "    params = {'objective': 'regression'}\n",
    "    d_train = lgb.Dataset(train_x, label=train_y)\n",
    "    model = lgb.train(params, d_train)\n",
    "    preds = model.predict(test_x)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    mse, mae, mape = evaluate_predictions(test_y, preds)\n",
    "\n",
    "    print(\"LightGBM\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "    print(f\"MAE: {mae:.3f}\")\n",
    "    print(f\"MAPE: {mape:.3f}\")\n",
    "    \n",
    "    return mse, mae, mape\n",
    "\n",
    "\n",
    "'''\n",
    "n_estimators:\n",
    "\n",
    "    The number of boosting stages to perform. Gradient boosting\n",
    "    is fairly robust to over-fitting so a large number usually\n",
    "    results in better performance.\n",
    "''' \n",
    "param_grid = {'n_estimators': [50,100,150,200,250,300,350,400]}\n",
    "model = GradientBoostingRegressor(random_state=seed)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(X_train, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model_params = []\n",
    "evaluation_results = []\n",
    "models = []\n",
    "for train_index, test_index in rfcv.split(rt_1_D_train):\n",
    "    train_data = rt_1_D_train.iloc[train_index]\n",
    "    test_data = rt_1_D_train.iloc[test_index]\n",
    "\n",
    "rfcv = RollingForecastCV(\n",
    "    h=170,  # forecast horizon\n",
    "    step=30,  # step size\n",
    ")\n",
    "\n",
    "##retrain model on whole dataset once done with cross validation\n",
    "\n",
    "#example with scores\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('knn', KNeighborsRegressor())])\n",
    "\n",
    "n_folds = 5\n",
    "n_neighbors = tuple(range(5, 101, 5))\n",
    "\n",
    "param_grid = {'knn__n_neighbors': n_neighbors}\n",
    "\n",
    "estimator = GridSearchCV(estimator=pipe,\n",
    "                         param_grid=param_grid,\n",
    "                         cv=n_folds,\n",
    "                         scoring=rmse_score,\n",
    "#                          n_jobs=-1\n",
    "                        )\n",
    "estimator.fit(X=X, y=y)\n",
    "cv_results = estimator.cv_results_\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple example\n",
    "# We do the same thing, but now instead for 12 months\n",
    "n_input = 12\n",
    "generator = TimeseriesGenerator(scaled_train, scaled_train, length=n_input, batch_size=1)\n",
    "     \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "     \n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "     \n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data geneartor\n",
    "# def create_dataset(dataset, look_back=1):\n",
    "#     data_x, data_y = [], []\n",
    "#     for i in range(len(dataset) - look_back):\n",
    "#         data_x.append(dataset[i:(i + look_back), 0])\n",
    "#         data_y.append(dataset[i + look_back, 0])\n",
    "#     return np.array(data_x), np.array(data_y)\n",
    "\n",
    "# def data_generator(data, window_size, batch_size):\n",
    "#     \"\"\"\n",
    "#     Generator function to create batches of input-output pairs for time series forecasting.\n",
    "#     :param data: A NumPy array containing the time series data.\n",
    "#     :param window_size: The size of the window used to create input-output pairs.\n",
    "#     :param batch_size: The number of input-output pairs per batch.\n",
    "#     :return: A generator that yields batches of input-output pairs.\n",
    "#     \"\"\"\n",
    "#     start = 0\n",
    "#     while True:\n",
    "#         if start + batch_size >= len(data) - window_size - 1:\n",
    "#             start = 0\n",
    "\n",
    "#         X_batch = np.zeros((batch_size, window_size, data.shape[1]))\n",
    "#         y_batch = np.zeros((batch_size, 2))\n",
    "\n",
    "#         for i in range(batch_size):\n",
    "#             X_batch[i] = data[start:start + window_size]\n",
    "#             y_batch[i] = data[start + window_size, :2]\n",
    "#             start += 1\n",
    "            \n",
    "#         # X_batch = np.transpose(X_batch, (0, 2, 1))\n",
    "#         # X_batch = np.where(np.isnan(X_batch), threshold+1, X_batch)\n",
    "        \n",
    "#         if np.any(y_batch==mask):\n",
    "#             continue\n",
    "        \n",
    "#         if np.count_nonzero(X_batch== mask)/X_batch.size > 0.5:\n",
    "#             continue\n",
    "\n",
    "#         yield X_batch, y_batch\n",
    "        \n",
    "# window_size = 120  # The size of the sliding window\n",
    "# batch_size = 32  # The number of samples per batch\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# train_split = int(0.8 * len(data))\n",
    "# train_data = data[:train_split]\n",
    "# val_split = int(0.8 * len(train_data))\n",
    "# val_data = train_data[val_split:]\n",
    "# train_data = train_data[:val_split]\n",
    "# test_data = data[train_split:]\n",
    "# print(train_data.shape, val_data.shape, test_data.shape)\n",
    "\n",
    "\n",
    "# # Create the training and validation generators\n",
    "# train_gen = data_generator(train_data, window_size, batch_size)\n",
    "# val_gen = data_generator(val_data, window_size, batch_size)\n",
    "# test_gen = data_generator(test_data, window_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build sirojjidin\n",
    "def get_compiled_model(train=False, lr = 1e-4):\n",
    "    \n",
    "    model = mlstm_fcn(2, input_shape = (window_size, 8))\n",
    "    # optm = Adam(learning_rate = lr, beta_1 = 0.9, beta_2 = 0.98, epsilon=1e-9)\n",
    "    optm = RMSprop(learning_rate=lr)\n",
    "    model.compile(loss = 'mse', \n",
    "                  optimizer=optm,\n",
    "                  metrics=[\"mae\", 'mse'])\n",
    "    return model\n",
    "model = get_compiled_model()\n",
    "model.summary()\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    steps_per_epoch = train_data.shape[0]//batch_size,\n",
    "    validation_steps = val_data.shape[0]//batch_size,\n",
    "    epochs=1,\n",
    "    batch_size=batch_size,\n",
    "    # callbacks=callback_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate test\n",
    "model.evaluate(test_gen, steps = test_data.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #build gneeral\n",
    "\n",
    "\n",
    "# def fit_lstm(train_x, train_y, test_x, test_y, lstm_units=50, look_back=1, num_epochs=10, batch_size=32):\n",
    "#     # Reshape input to be [samples, time steps, features]\n",
    "#     train_x = train_x.reshape((train_x.shape[0], look_back, 1))\n",
    "#     test_x = test_x.reshape((test_x.shape[0], look_back, 1))\n",
    "\n",
    "#     # Build LSTM model\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(8, input_shape=(look_back, 1), return_sequences=True))\n",
    "#     model.add(LSTM(4))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mape'])\n",
    "\n",
    "#     # Fit the model\n",
    "#     model.fit(train_x, train_y, epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "#     # Make predictions\n",
    "#     train_pred = model.predict(train_x)\n",
    "#     test_pred = model.predict(test_x)\n",
    "\n",
    "#     # Inverse scaling of the predictions\n",
    "#     train_pred = scaler.inverse_transform(train_pred)\n",
    "#     train_y = scaler.inverse_transform(train_y)\n",
    "#     test_pred = scaler.inverse_transform(test_pred)\n",
    "#     test_y = scaler.inverse_transform(test_y)\n",
    "\n",
    "#     # Create a DataFrame of the predictions and actual values\n",
    "#     train_results = pd.DataFrame({'Actual': train_y.flatten(), 'Predicted': train_pred.flatten()}, index=train_data.index[1:])\n",
    "#     test_results = pd.DataFrame({'Actual': test_y.flatten(), 'Predicted': test_pred.flatten()}, index=test_data.index[1:])\n",
    "\n",
    "#     # Evaluate the predictions\n",
    "#     train_mse, train_mae, train_mape = evaluate_predictions(train_y, train_pred)\n",
    "#     test_mse, test_mae, test_mape = evaluate_predictions(test_y, test_pred)\n",
    "    \n",
    "#     # Evaluate the model on the test set\n",
    "#     loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     print(f\"Test loss: {loss}\")\n",
    "\n",
    "#     # Print the evaluation metrics\n",
    "#     print(\"Train MSE: {:.3f}, Train MAE: {:.3f}, Train MAPE: {:.3f}\".format(train_mse, train_mae, train_mape))\n",
    "#     print(\"Test MSE: {:.3f}, Test MAE: {:.3f}, Test MAPE: {:.3f}\".format(test_mse, test_mae, test_mape))\n",
    "\n",
    "#     return pd.concat([train_results, test_results])\n",
    "\n",
    "\n",
    "\n",
    "# def build_lstm_model(units=50, activation='relu', optimizer='adam'):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(units=units, activation=activation))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "#     return model\n",
    "\n",
    "# def tune_lstm(X_train, y_train):\n",
    "#     param_grid = {\n",
    "#         'units': [50, 100],\n",
    "#         'activation': ['relu', 'tanh'],\n",
    "#         'optimizer': [Adam(learning_rate=0.001), Adam(learning_rate=0.01)]\n",
    "#     }\n",
    "#     model = KerasRegressor(build_fn=build_lstm_model, epochs=50, batch_size=32, verbose=0)\n",
    "#     tscv = TimeSeriesSplit(n_splits=5)\n",
    "#     grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
    "#     grid_result = grid.fit(X_train, y_train)\n",
    "#     best_params = grid_result.best_params_\n",
    "#     best_lstm_model = build_lstm_model(**best_params)\n",
    "#     best_lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "    \n",
    "#     print(f\"Best LSTM model with parameters {best_params}\")\n",
    "#     return best_lstm_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #importing required libraries\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# #creating dataframe\n",
    "# data = df.sort_index(ascending=True, axis=0)\n",
    "# new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "# for i in range(0,len(data)):\n",
    "#     new_data['Date'][i] = data['Date'][i]\n",
    "#     new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "# #setting index\n",
    "# new_data.index = new_data.Date\n",
    "# new_data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# #creating train and test sets\n",
    "# dataset = new_data.values\n",
    "\n",
    "# train = dataset[0:987,:]\n",
    "# valid = dataset[987:,:]\n",
    "\n",
    "# #converting dataset into x_train and y_train\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# x_train, y_train = [], []\n",
    "# for i in range(60,len(train)):\n",
    "#     x_train.append(scaled_data[i-60:i,0])\n",
    "#     y_train.append(scaled_data[i,0])\n",
    "# x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "\n",
    "# # create and fit the LSTM network\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1],1)))\n",
    "# model.add(LSTM(units=50))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# model.fit(x_train, y_train, epochs=1, batch_size=1, verbose=2)\n",
    "\n",
    "# #predicting 246 values, using past 60 from the train data\n",
    "# inputs = new_data[len(new_data) - len(valid) - 60:].values\n",
    "# inputs = inputs.reshape(-1,1)\n",
    "# inputs  = scaler.transform(inputs)\n",
    "\n",
    "# X_test = []\n",
    "# for i in range(60,inputs.shape[0]):\n",
    "#     X_test.append(inputs[i-60:i,0])\n",
    "# X_test = np.array(X_test)\n",
    "\n",
    "# X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "# closing_price = model.predict(X_test)\n",
    "# closing_price = scaler.inverse_transform(closing_price)\n",
    "# Results\n",
    "# rms=np.sqrt(np.mean(np.power((valid-closing_price),2)))\n",
    "# rms\n",
    "\n",
    "# #for plotting\n",
    "# train = new_data[:987]\n",
    "# valid = new_data[987:]\n",
    "# valid['Predictions'] = closing_price\n",
    "# plt.plot(train['Close'])\n",
    "# plt.plot(valid[['Close','Predictions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Creating the scaled training data set\n",
    "# train_data = scaled_data[0:training_data_len  , : ]\n",
    "# #Spliting the data into x_train and y_train data sets\n",
    "# x_train=[]\n",
    "# y_train = []\n",
    "# for i in range(60,len(train_data)):\n",
    "#     x_train.append(train_data[i-60:i,0])\n",
    "#     y_train.append(train_data[i,0])\n",
    "#     if i<= 61:\n",
    "#         print(x_train)\n",
    "#         print(y_train)\n",
    "#         print()\n",
    "# #Here we are Converting x_train and y_train to numpy arrays\n",
    "# x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "# # Here we are reshaping the data into the shape accepted by the LSTM\n",
    "# x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "# #now we are Building the LSTM network model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, return_sequences=True,input_shape=(x_train.shape[1],1)))\n",
    "# model.add(LSTM(units=50, return_sequences=False))\n",
    "# model.add(Dense(units=25))\n",
    "# model.add(Dense(units=1))\n",
    "# # here we are Compiling the model\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# # here we are training the model\n",
    "# model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "# 1756/1756 [==============================] - 78s 44ms/step - loss: 2.9184e-04\n",
    "# <tensorflow.python.keras.callbacks.History at 0x7f24c4477670>\n",
    "# # here we are testing data set\n",
    "# test_data = scaled_data[training_data_len - 60: , : ]\n",
    "# #Creating the x_test and y_test data sets\n",
    "# x_test = []\n",
    "# y_test =  dataset[training_data_len : , : ] #Get all of the rows from index 1603 to the rest and all of the columns (in this case it's only column 'Close'), so 2003 - 1603 = 400 rows of data\n",
    "# for i in range(60,len(test_data)):\n",
    "#     x_test.append(test_data[i-60:i,0])\n",
    "# # here we are converting x_test to a numpy array  \n",
    "# x_test = np.array(x_test)\n",
    "# # here we are reshaping the data into the shape accepted by the LSTM  \n",
    "# x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
    "# # now we are getting the models predicted price values\n",
    "# predictions = model.predict(x_test) \n",
    "# predictions = scaler.inverse_transform(predictions)#Undo scaling\n",
    "# # here we are calculaing the value of RMSE \n",
    "# rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "# rmse\n",
    "# # here we are plotting the data\n",
    "# #Plot/Create the data for the graph\n",
    "# train = data[:training_data_len]\n",
    "# valid = data[training_data_len:]\n",
    "# valid['Predictions'] = predictions\n",
    "# #Visualize the data\n",
    "# plt.figure(figsize=(16,8))\n",
    "# plt.title('Model')\n",
    "# plt.xlabel('Date', fontsize=18)\n",
    "# plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "# plt.plot(train['Close'])\n",
    "# plt.plot(valid[['Close', 'Predictions']])\n",
    "# plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get stacked LSTM model for regression modeling\n",
    "# def get_reg_model(layer_units=[100,100],dropouts=[0.2,0.2],window_size=50):\n",
    "#     # build LSTM network\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # hidden layer 1\n",
    "#     model.add(LSTM(layer_units[0], \n",
    "#                    input_shape=(window_size,1), \n",
    "#                    return_sequences=True))\n",
    "#     model.add(Dropout(dropouts[0]))\n",
    "    \n",
    "#     # hidden layer 2\n",
    "#     model.add(LSTM(layer_units[1]))\n",
    "#     model.add(Dropout(dropouts[1]))\n",
    "    \n",
    "#     # output layer\n",
    "#     model.add(Dense(1))\n",
    "#     model.add(Activation(\"linear\"))\n",
    "    \n",
    "#     start = time.time()\n",
    "#     model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "#     print(\"> Compilation Time : \", time.time() - start)\n",
    "#     print(model.summary())\n",
    "#     return model\n",
    "\n",
    "# # Window wise prediction function\n",
    "# def predict_reg_multiple(model, data, window_size=6, prediction_len=3):\n",
    "#     prediction_list = []\n",
    "    \n",
    "#     # loop for every sequence in the dataset\n",
    "#     for window in range(int(len(data)/prediction_len)):\n",
    "#         _seq = data[window*prediction_len]\n",
    "#         predicted = []\n",
    "#         # loop till required prediction length is achieved\n",
    "#         for j in range(prediction_len):\n",
    "#             predicted.append(model.predict(_seq[np.newaxis,:,:])[0,0])\n",
    "#             _seq = _seq[1:]\n",
    "#             _seq = np.insert(_seq, [window_size-1], predicted[-1], axis=0)\n",
    "#         prediction_list.append(predicted)\n",
    "#     return prediction_list\n",
    "\n",
    "\n",
    "# # Plot window wise \n",
    "# def plot_reg_results(predicted_data, true_data, prediction_len=3):\n",
    "#     fig = plt.figure(facecolor='white')\n",
    "#     ax = fig.add_subplot(111)\n",
    "    \n",
    "#     # plot actual data\n",
    "#     ax.plot(true_data, \n",
    "#             label='True Data',\n",
    "#             c='black',alpha=0.3)\n",
    "    \n",
    "#     # plot flattened data\n",
    "#     plt.plot(np.array(predicted_data).flatten(), \n",
    "#              label='Prediction_full',\n",
    "#              c='g',linestyle='--')\n",
    "    \n",
    "#     #plot each window in the prediction list\n",
    "#     for i, data in enumerate(predicted_data):\n",
    "#         padding = [None for p in range(i * prediction_len)]\n",
    "#         plt.plot(padding + data, label='Prediction',c='black')\n",
    "\n",
    "#     plt.title(\"Forecast Plot with Prediction Window={}\".format(prediction_len))\n",
    "#     plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(100, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(layers.LSTM(100, return_sequences=False))\n",
    "model.add(layers.Dense(25))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(x_train, y_train, batch_size= 1, epochs=3)\n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "rmse = np.sqrt(np.mean(predictions - y_test)**2)\n",
    "rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM  optimsation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Running deep learning models and performing cross validation takes time\n",
    "# #Set the following Flag to 0 if the Deep LEarning Models Flag has to be disabled\n",
    "# EnableDeepLearningRegreesorFlag = 0\n",
    "\n",
    "# def create_model(neurons=12, activation='relu', learn_rate = 0.01, momentum=0):\n",
    "#         # create model\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n",
    "#         #The number of hidden layers can be increased\n",
    "#         model.add(Dense(2, activation=activation))\n",
    "#         # Final output layer\n",
    "#         model.add(Dense(1, kernel_initializer='normal'))\n",
    "#         # Compile model\n",
    "#         optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "#         model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "#         return model  \n",
    "#     #Add Deep Learning Regressor\n",
    "# if ( EnableDeepLearningRegreesorFlag == 1):\n",
    "#     models.append(('DNN', KerasRegressor(build_fn=create_model, epochs=100, batch_size=100, verbose=1)))  \n",
    "    \n",
    "# X_train_LSTM, X_validation_LSTM = np.array(X_train), np.array(X_validation)\n",
    "# Y_train_LSTM, Y_validation_LSTM = np.array(Y_train), np.array(Y_validation)\n",
    "# X_train_LSTM= X_train_LSTM.reshape((X_train_LSTM.shape[0], 1, X_train_LSTM.shape[1]))\n",
    "# X_validation_LSTM= X_validation_LSTM.reshape((X_validation_LSTM.shape[0], 1, X_validation_LSTM.shape[1]))\n",
    "# print(X_train_LSTM.shape, Y_train_LSTM.shape, X_validation_LSTM.shape, Y_validation_LSTM.shape)   \n",
    "\n",
    "\n",
    "# def create_LSTMmodel(neurons=12, learn_rate = 0.01, momentum=0):\n",
    "#         # create model\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1], X_train_LSTM.shape[2])))\n",
    "#     #More number of cells can be added if needed \n",
    "#     model.add(Dense(1))\n",
    "#     optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "#     model.compile(loss='mse', optimizer='adam')\n",
    "#     return model\n",
    "# LSTMModel = create_LSTMmodel(12, learn_rate = 0.01, momentum=0)\n",
    "# LSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM, validation_data=(X_validation_LSTM, Y_validatio\n",
    "    \n",
    "# #Visual plot to check if the error is reducing\n",
    "# pyplot.plot(LSTMModel_fit.history['loss'], label='train')\n",
    "# pyplot.plot(LSTMModel_fit.history['val_loss'], label='test')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Grid Search for LSTM Model\n",
    "\n",
    "# # evaluate an LSTM model for a given order (p,d,q)\n",
    "# def evaluate_LSTM_model(neurons=12, learn_rate = 0.01, momentum=0):\n",
    "#     #predicted = list()     \n",
    "#     LSTMModel = create_LSTMmodel(neurons, learn_rate, momentum)\n",
    "#     LSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM,epochs=50, batch_size=72, verbose=0, shuffle=False)\n",
    "#     predicted = LSTMModel.predict(X_validation_LSTM)\n",
    "#     error = mean_squared_error(predicted, Y_validation)\n",
    "#     return error\n",
    "\n",
    "# # evaluate combinations of different variables of LSTM Model\n",
    "# def evaluate_combinations_LSTM(neurons, learn_rate, momentum): \n",
    "#     best_score, best_cfg = float(\"inf\"), None\n",
    "#     for n in neurons:\n",
    "#         for l in learn_rate:\n",
    "#             for m in momentum:\n",
    "#                 combination = (n,l,m)                \n",
    "#                 try:\n",
    "#                     mse = evaluate_LSTM_model(n,l,m)                    \n",
    "#                     if mse < best_score:\n",
    "#                         best_score, best_cfg = mse, combination\n",
    "#                     print('LSTM%s MSE=%.7f' % (combination,mse))\n",
    "#                 except:\n",
    "#                     continue\n",
    "#     print('Best LSTM%s MSE=%.7f' % (best_cfg, best_score))\n",
    "    \n",
    "# # evaluate parameters\n",
    "# neurons = [1, 5]\n",
    "# learn_rate = [0.001, 0.3]\n",
    "# momentum = [0.0, 0.9]\n",
    "# #Other Parameters can be modified as well\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# epochs = [10, 50, 100]\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# evaluate_combinations_LSTM(neurons,learn_rate,momentum)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ########\n",
    "# #grid search example\n",
    "# # transform list into supervised learning format\n",
    "# def series_to_supervised(data, n_in=1, n_out=1):\n",
    "#  df = DataFrame(data)\n",
    "#  cols = list()\n",
    "#  # input sequence (t-n, ... t-1)\n",
    "#  for i in range(n_in, 0, -1):\n",
    "#  cols.append(df.shift(i))\n",
    "#  # forecast sequence (t, t+1, ... t+n)\n",
    "#  for i in range(0, n_out):\n",
    "#  cols.append(df.shift(-i))\n",
    "#  # put it all together\n",
    "#  agg = concat(cols, axis=1)\n",
    "#  # drop rows with NaN values\n",
    "#  agg.dropna(inplace=True)\n",
    "#  return agg.values\n",
    " \n",
    "# # root mean squared error or rmse\n",
    "# def measure_rmse(actual, predicted):\n",
    "#  return sqrt(mean_squared_error(actual, predicted))\n",
    " \n",
    "# # difference dataset\n",
    "# def difference(data, order):\n",
    "#  return [data[i] - data[i - order] for i in range(order, len(data))]\n",
    " \n",
    "# # fit a model\n",
    "# def model_fit(train, config):\n",
    "#  # unpack config\n",
    "#  n_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
    "#  # prepare data\n",
    "#  if n_diff > 0:\n",
    "#  train = difference(train, n_diff)\n",
    "#  # transform series into supervised format\n",
    "#  data = series_to_supervised(train, n_in=n_input)\n",
    "#  # separate inputs and outputs\n",
    "#  train_x, train_y = data[:, :-1], data[:, -1]\n",
    "#  # reshape input data into [samples, timesteps, features]\n",
    "#  n_features = 1\n",
    "#  train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
    "#  # define model\n",
    "#  model = Sequential()\n",
    "#  model.add(LSTM(n_nodes, activation='relu', input_shape=(n_input, n_features)))\n",
    "#  model.add(Dense(n_nodes, activation='relu'))\n",
    "#  model.add(Dense(1))\n",
    "#  model.compile(loss='mse', optimizer='adam')\n",
    "#  # fit model\n",
    "#  model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "#  return model\n",
    " \n",
    "# # forecast with the fit model\n",
    "# def model_predict(model, history, config):\n",
    "#  # unpack config\n",
    "#  n_input, _, _, _, n_diff = config\n",
    "#  # prepare data\n",
    "#  correction = 0.0\n",
    "#  if n_diff > 0:\n",
    "#  correction = history[-n_diff]\n",
    "#  history = difference(history, n_diff)\n",
    "#  # reshape sample into [samples, timesteps, features]\n",
    "#  x_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "#  # forecast\n",
    "#  yhat = model.predict(x_input, verbose=0)\n",
    "#  return correction + yhat[0]\n",
    " \n",
    "# # walk-forward validation for univariate data\n",
    "# def walk_forward_validation(data, n_test, cfg):\n",
    "#  predictions = list()\n",
    "#  # split dataset\n",
    "#  train, test = train_test_split(data, n_test)\n",
    "#  # fit model\n",
    "#  model = model_fit(train, cfg)\n",
    "#  # seed history with training dataset\n",
    "#  history = [x for x in train]\n",
    "#  # step over each time-step in the test set\n",
    "#  for i in range(len(test)):\n",
    "#  # fit model and make forecast for history\n",
    "#  yhat = model_predict(model, history, cfg)\n",
    "#  # store forecast in list of predictions\n",
    "#  predictions.append(yhat)\n",
    "#  # add actual observation to history for the next loop\n",
    "#  history.append(test[i])\n",
    "#  # estimate prediction error\n",
    "#  error = measure_rmse(test, predictions)\n",
    "#  print(' > %.3f' % error)\n",
    "#  return error\n",
    " \n",
    "# # score a model, return None on failure\n",
    "# def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "#  # convert config to a key\n",
    "#  key = str(config)\n",
    "#  # fit and evaluate the model n times\n",
    "#  scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "#  # summarize score\n",
    "#  result = mean(scores)\n",
    "#  print('> Model[%s] %.3f' % (key, result))\n",
    "#  return (key, result)\n",
    " \n",
    "# # grid search configs\n",
    "# def grid_search(data, cfg_list, n_test):\n",
    "#  # evaluate configs\n",
    "#  scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "#  # sort configs by error, asc\n",
    "#  scores.sort(key=lambda tup: tup[1])\n",
    "#  return scores\n",
    " \n",
    "# # create a list of configs to try\n",
    "# def model_configs():\n",
    "#  # define scope of configs\n",
    "#  n_input = [12]\n",
    "#  n_nodes = [100]\n",
    "#  n_epochs = [50]\n",
    "#  n_batch = [1, 150]\n",
    "#  n_diff = [12]\n",
    "#  # create configs\n",
    "#  configs = list()\n",
    "#  for i in n_input:\n",
    "#  for j in n_nodes:\n",
    "#  for k in n_epochs:\n",
    "#  for l in n_batch:\n",
    "#  for m in n_diff:\n",
    "#  cfg = [i, j, k, l, m]\n",
    "#  configs.append(cfg)\n",
    "#  print('Total configs: %d' % len(configs))\n",
    "#  return configs\n",
    " \n",
    "# # define dataset\n",
    "# series = read_csv('monthly-airline-passengers.csv', header=0, index_col=0)\n",
    "# data = series.values\n",
    "# # data split\n",
    "# n_test = 12\n",
    "# # model configs\n",
    "# cfg_list = model_configs()\n",
    "# # grid search\n",
    "# scores = grid_search(data, cfg_list, n_test)\n",
    "# print('done')\n",
    "# # list top 10 configs\n",
    "# for cfg, error in scores[:3]:\n",
    "#  print(cfg, error)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    #converted mean square error to positive. The lower the beter\n",
    "    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "    \n",
    "    \n",
    "## all model results\n",
    "names = []\n",
    "kfold_results = []\n",
    "test_results = []\n",
    "train_results = []\n",
    "for name, model in models:\n",
    "    names.append(name)\n",
    "    \n",
    "    ## K Fold analysis:\n",
    "    \n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    #converted mean square error to positive. The lower the beter\n",
    "    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    kfold_results.append(cv_results)\n",
    "    \n",
    "\n",
    "    # Full Training period\n",
    "    res = model.fit(X_train, Y_train)\n",
    "    train_result = mean_squared_error(res.predict(X_train), Y_train)\n",
    "    train_results.append(train_result)\n",
    "    \n",
    "    # Test results\n",
    "    test_result = mean_squared_error(res.predict(X_test), Y_test)\n",
    "    test_results.append(test_result)\n",
    "    \n",
    "    msg = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), cv_results.std(), train_result, test_result)\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "#results\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison: Kfold results')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(kfold_results)\n",
    "ax.set_xticklabels(names)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "# compare algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()\n",
    "\n",
    "#train test erro\n",
    "# compare algorithms\n",
    "fig = pyplot.figure()\n",
    "\n",
    "ind = np.arange(len(names))  # the x locations for the groups\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.bar(ind - width/2, train_results,  width=width, label='Train Error')\n",
    "pyplot.bar(ind + width/2, test_results, width=width, label='Test Error')\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.legend()\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(names)\n",
    "pyplot.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare algorithms\n",
    "fig = pyplot.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "pyplot.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "fig.set_size_inches(15,8)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOrecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "#scaler = StandardScaler().fit(X_train)\n",
    "#rescaledX = scaler.transform(X_train)\n",
    "model = RandomForestRegressor(n_estimators=250) # rbf is default kernel\n",
    "model.fit(X_train, Y_train)\n",
    "# estimate accuracy on validation set\n",
    "# transform the validation dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "#rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions = model.predict(X_validation)\n",
    "print(mean_squared_error(Y_validation, predictions))\n",
    "print(r2_score(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_lagged_features(series, lag=1):\n",
    "#     # Create a DataFrame from the input series\n",
    "#     df = pd.DataFrame(series)\n",
    "    \n",
    "#     # Create lagged features\n",
    "#     for i in range(1, lag + 1):\n",
    "#         df[f\"lag_{i}\"] = df.shift(i)\n",
    "    \n",
    "#     # Drop rows with missing values caused by shifting\n",
    "#     df.dropna(inplace=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "    \n",
    "\n",
    "# >>> import statsmodels.api as sm\n",
    "# >>> data = sm.datasets.macrodata.load()\n",
    "# >>> data = data.data[['year','quarter','realgdp','cpi']]\n",
    "# >>> data = sm.tsa.add_lag(data, 'realgdp', lags=2)\n",
    "\n",
    "def series_to_supervised(data, lag=1):\n",
    "    n_vars = data.shape[1]\n",
    "    df = pd.DataFrame(data)    \n",
    "    cols, names = list(), list()\n",
    "    for i in range(lag, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('%s(t-%d)' % (df.columns[j], i)) for j in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    agg = pd.DataFrame(data.iloc[:,0]).join(agg)\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg\n",
    "dataset= series_to_supervised(dataset,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
